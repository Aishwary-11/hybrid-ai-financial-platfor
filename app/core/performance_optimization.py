#!/usr/bin/env python3\n\"\"\"\nPerformance Optimization System\nIntelligent caching, load balancing, auto-scaling, and resource optimization\n\"\"\"\n\nimport asyncio\nimport numpy as np\nimport pandas as pd\nfrom typing import Dict, List, Optional, Any, Tuple, Union, Callable\nfrom datetime import datetime, timedelta\nimport json\nimport logging\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nfrom abc import ABC, abstractmethod\nimport hashlib\nfrom collections import defaultdict, deque\nimport time\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport redis\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom app.core.hybrid_ai_engine import ModelOutput, TaskCategory\n\nlogger = logging.getLogger(__name__)\n\n\nclass CacheStrategy(Enum):\n    \"\"\"Caching strategies\"\"\"\n    LRU = \"lru\"  # Least Recently Used\n    LFU = \"lfu\"  # Least Frequently Used\n    TTL = \"ttl\"  # Time To Live\n    ADAPTIVE = \"adaptive\"  # Adaptive based on usage patterns\n    SEMANTIC = \"semantic\"  # Semantic similarity based\n\n\nclass ScalingStrategy(Enum):\n    \"\"\"Auto-scaling strategies\"\"\"\n    REACTIVE = \"reactive\"  # React to current load\n    PREDICTIVE = \"predictive\"  # Predict future load\n    HYBRID = \"hybrid\"  # Combination of reactive and predictive\n    COST_AWARE = \"cost_aware\"  # Consider cost in scaling decisions\n\n\nclass OptimizationObjective(Enum):\n    \"\"\"Optimization objectives\"\"\"\n    MINIMIZE_LATENCY = \"minimize_latency\"\n    MINIMIZE_COST = \"minimize_cost\"\n    MAXIMIZE_THROUGHPUT = \"maximize_throughput\"\n    BALANCE_ALL = \"balance_all\"\n    MAXIMIZE_ACCURACY = \"maximize_accuracy\"\n\n\n@dataclass\nclass CacheEntry:\n    \"\"\"Cache entry with metadata\"\"\"\n    key: str\n    value: Any\n    created_at: datetime\n    last_accessed: datetime\n    access_count: int\n    ttl_seconds: Optional[int]\n    size_bytes: int\n    hit_rate: float\n    metadata: Dict[str, Any]\n\n\n@dataclass\nclass PerformanceMetrics:\n    \"\"\"Performance metrics for monitoring\"\"\"\n    timestamp: datetime\n    response_time: float\n    throughput: float\n    error_rate: float\n    cache_hit_rate: float\n    resource_utilization: Dict[str, float]\n    cost_per_request: float\n    queue_length: int\n    active_connections: int\n    memory_usage: float\n    cpu_usage: float\n\n\n@dataclass\nclass ScalingDecision:\n    \"\"\"Auto-scaling decision\"\"\"\n    timestamp: datetime\n    current_instances: int\n    target_instances: int\n    scaling_reason: str\n    confidence: float\n    estimated_cost_impact: float\n    estimated_performance_impact: float\n    metadata: Dict[str, Any]\n\n\nclass IntelligentCache:\n    \"\"\"Intelligent caching system with multiple strategies\"\"\"\n    \n    def __init__(self, max_size: int = 10000, default_ttl: int = 3600,\n                 strategy: CacheStrategy = CacheStrategy.ADAPTIVE,\n                 redis_client: Optional[redis.Redis] = None):\n        self.max_size = max_size\n        self.default_ttl = default_ttl\n        self.strategy = strategy\n        self.redis_client = redis_client\n        \n        # In-memory cache\n        self.cache: Dict[str, CacheEntry] = {}\n        self.access_order = deque()  # For LRU\n        self.access_frequency = defaultdict(int)  # For LFU\n        \n        # Performance tracking\n        self.hit_count = 0\n        self.miss_count = 0\n        self.total_requests = 0\n        \n        # Semantic similarity cache (for semantic strategy)\n        self.semantic_embeddings: Dict[str, np.ndarray] = {}\n        \n        # Background cleanup thread\n        self.cleanup_thread = threading.Thread(target=self._cleanup_expired, daemon=True)\n        self.cleanup_thread.start()\n        \n        logger.info(f\"Intelligent Cache initialized with strategy: {strategy.value}\")\n    \n    async def get(self, key: str, compute_func: Optional[Callable] = None) -> Optional[Any]:\n        \"\"\"Get value from cache or compute if not found\"\"\"\n        \n        self.total_requests += 1\n        \n        # Check in-memory cache first\n        if key in self.cache:\n            entry = self.cache[key]\n            \n            # Check TTL\n            if self._is_expired(entry):\n                await self.delete(key)\n            else:\n                # Update access metadata\n                entry.last_accessed = datetime.now()\n                entry.access_count += 1\n                self.access_frequency[key] += 1\n                \n                # Update LRU order\n                if key in self.access_order:\n                    self.access_order.remove(key)\n                self.access_order.append(key)\n                \n                self.hit_count += 1\n                logger.debug(f\"Cache hit for key: {key}\")\n                return entry.value\n        \n        # Check Redis cache if available\n        if self.redis_client:\n            try:\n                redis_value = self.redis_client.get(key)\n                if redis_value:\n                    value = pickle.loads(redis_value)\n                    # Store in local cache too\n                    await self.set(key, value)\n                    self.hit_count += 1\n                    logger.debug(f\"Redis cache hit for key: {key}\")\n                    return value\n            except Exception as e:\n                logger.warning(f\"Redis cache error: {e}\")\n        \n        # Cache miss - compute if function provided\n        self.miss_count += 1\n        \n        if compute_func:\n            try:\n                value = await compute_func() if asyncio.iscoroutinefunction(compute_func) else compute_func()\n                await self.set(key, value)\n                logger.debug(f\"Cache miss, computed and stored: {key}\")\n                return value\n            except Exception as e:\n                logger.error(f\"Error computing value for key {key}: {e}\")\n                return None\n        \n        logger.debug(f\"Cache miss for key: {key}\")\n        return None\n    \n    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:\n        \"\"\"Set value in cache\"\"\"\n        \n        ttl = ttl or self.default_ttl\n        \n        # Calculate size (approximate)\n        try:\n            size_bytes = len(pickle.dumps(value))\n        except:\n            size_bytes = 1024  # Default estimate\n        \n        # Create cache entry\n        entry = CacheEntry(\n            key=key,\n            value=value,\n            created_at=datetime.now(),\n            last_accessed=datetime.now(),\n            access_count=1,\n            ttl_seconds=ttl,\n            size_bytes=size_bytes,\n            hit_rate=0.0,\n            metadata={}\n        )\n        \n        # Check if we need to evict entries\n        if len(self.cache) >= self.max_size:\n            await self._evict_entries()\n        \n        # Store in local cache\n        self.cache[key] = entry\n        self.access_order.append(key)\n        self.access_frequency[key] += 1\n        \n        # Store in Redis if available\n        if self.redis_client:\n            try:\n                self.redis_client.setex(key, ttl, pickle.dumps(value))\n            except Exception as e:\n                logger.warning(f\"Redis set error: {e}\")\n        \n        logger.debug(f\"Cached value for key: {key}\")\n        return True\n    \n    async def delete(self, key: str) -> bool:\n        \"\"\"Delete value from cache\"\"\"\n        \n        # Remove from local cache\n        if key in self.cache:\n            del self.cache[key]\n        \n        if key in self.access_order:\n            self.access_order.remove(key)\n        \n        if key in self.access_frequency:\n            del self.access_frequency[key]\n        \n        # Remove from Redis\n        if self.redis_client:\n            try:\n                self.redis_client.delete(key)\n            except Exception as e:\n                logger.warning(f\"Redis delete error: {e}\")\n        \n        logger.debug(f\"Deleted cache entry: {key}\")\n        return True\n    \n    async def get_similar(self, query: str, similarity_threshold: float = 0.8) -> Optional[Any]:\n        \"\"\"Get cached value based on semantic similarity\"\"\"\n        \n        if self.strategy != CacheStrategy.SEMANTIC:\n            return None\n        \n        # This is a simplified implementation\n        # In practice, you'd use proper embeddings and similarity computation\n        \n        query_embedding = self._compute_embedding(query)\n        \n        best_similarity = 0.0\n        best_key = None\n        \n        for key in self.cache.keys():\n            if key in self.semantic_embeddings:\n                similarity = self._cosine_similarity(\n                    query_embedding, self.semantic_embeddings[key]\n                )\n                \n                if similarity > best_similarity and similarity >= similarity_threshold:\n                    best_similarity = similarity\n                    best_key = key\n        \n        if best_key:\n            logger.debug(f\"Semantic cache hit: {query} -> {best_key} (similarity: {best_similarity:.3f})\")\n            return await self.get(best_key)\n        \n        return None\n    \n    async def _evict_entries(self):\n        \"\"\"Evict entries based on strategy\"\"\"\n        \n        if self.strategy == CacheStrategy.LRU:\n            # Remove least recently used\n            while len(self.cache) >= self.max_size and self.access_order:\n                oldest_key = self.access_order.popleft()\n                if oldest_key in self.cache:\n                    del self.cache[oldest_key]\n                    if oldest_key in self.access_frequency:\n                        del self.access_frequency[oldest_key]\n        \n        elif self.strategy == CacheStrategy.LFU:\n            # Remove least frequently used\n            if self.access_frequency:\n                lfu_key = min(self.access_frequency.items(), key=lambda x: x[1])[0]\n                await self.delete(lfu_key)\n        \n        elif self.strategy == CacheStrategy.TTL:\n            # Remove expired entries first\n            expired_keys = []\n            for key, entry in self.cache.items():\n                if self._is_expired(entry):\n                    expired_keys.append(key)\n            \n            for key in expired_keys:\n                await self.delete(key)\n        \n        elif self.strategy == CacheStrategy.ADAPTIVE:\n            # Adaptive eviction based on hit rate and recency\n            scored_entries = []\n            \n            for key, entry in self.cache.items():\n                age_hours = (datetime.now() - entry.created_at).total_seconds() / 3600\n                recency_score = 1.0 / (1.0 + age_hours)\n                frequency_score = self.access_frequency.get(key, 1) / max(self.access_frequency.values())\n                \n                # Combined score (lower is worse)\n                score = recency_score * 0.6 + frequency_score * 0.4\n                scored_entries.append((score, key))\n            \n            # Remove lowest scoring entries\n            scored_entries.sort()\n            entries_to_remove = len(self.cache) - self.max_size + 1\n            \n            for i in range(min(entries_to_remove, len(scored_entries))):\n                key = scored_entries[i][1]\n                await self.delete(key)\n    \n    def _is_expired(self, entry: CacheEntry) -> bool:\n        \"\"\"Check if cache entry is expired\"\"\"\n        \n        if entry.ttl_seconds is None:\n            return False\n        \n        age_seconds = (datetime.now() - entry.created_at).total_seconds()\n        return age_seconds > entry.ttl_seconds\n    \n    def _cleanup_expired(self):\n        \"\"\"Background thread to cleanup expired entries\"\"\"\n        \n        while True:\n            try:\n                expired_keys = []\n                for key, entry in self.cache.items():\n                    if self._is_expired(entry):\n                        expired_keys.append(key)\n                \n                for key in expired_keys:\n                    asyncio.create_task(self.delete(key))\n                \n                time.sleep(300)  # Check every 5 minutes\n                \n            except Exception as e:\n                logger.error(f\"Cache cleanup error: {e}\")\n                time.sleep(60)\n    \n    def _compute_embedding(self, text: str) -> np.ndarray:\n        \"\"\"Compute text embedding (simplified)\"\"\"\n        \n        # This is a very simplified embedding\n        # In practice, use proper embeddings like sentence-transformers\n        \n        words = text.lower().split()\n        embedding = np.zeros(100)  # 100-dimensional embedding\n        \n        for i, word in enumerate(words[:10]):  # Use first 10 words\n            word_hash = hash(word) % 100\n            embedding[word_hash] += 1.0 / (i + 1)  # Position weighting\n        \n        # Normalize\n        norm = np.linalg.norm(embedding)\n        if norm > 0:\n            embedding = embedding / norm\n        \n        return embedding\n    \n    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:\n        \"\"\"Compute cosine similarity between two vectors\"\"\"\n        \n        dot_product = np.dot(a, b)\n        norm_a = np.linalg.norm(a)\n        norm_b = np.linalg.norm(b)\n        \n        if norm_a == 0 or norm_b == 0:\n            return 0.0\n        \n        return dot_product / (norm_a * norm_b)\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics\"\"\"\n        \n        hit_rate = self.hit_count / self.total_requests if self.total_requests > 0 else 0.0\n        \n        total_size = sum(entry.size_bytes for entry in self.cache.values())\n        avg_entry_size = total_size / len(self.cache) if self.cache else 0\n        \n        return {\n            'total_entries': len(self.cache),\n            'max_size': self.max_size,\n            'hit_rate': hit_rate,\n            'hit_count': self.hit_count,\n            'miss_count': self.miss_count,\n            'total_requests': self.total_requests,\n            'total_size_bytes': total_size,\n            'average_entry_size': avg_entry_size,\n            'strategy': self.strategy.value,\n            'utilization': len(self.cache) / self.max_size\n        }\n\n\nclass LoadBalancer:\n    \"\"\"Advanced load balancer with multiple algorithms\"\"\"\n    \n    def __init__(self):\n        self.servers: Dict[str, Dict[str, Any]] = {}\n        self.request_counts = defaultdict(int)\n        self.response_times = defaultdict(list)\n        self.health_status = defaultdict(bool)\n        self.weights = defaultdict(lambda: 1.0)\n        \n        # Load balancing algorithms\n        self.algorithms = {\n            'round_robin': self._round_robin,\n            'weighted_round_robin': self._weighted_round_robin,\n            'least_connections': self._least_connections,\n            'least_response_time': self._least_response_time,\n            'weighted_least_connections': self._weighted_least_connections,\n            'ip_hash': self._ip_hash,\n            'adaptive': self._adaptive_selection\n        }\n        \n        self.current_algorithm = 'adaptive'\n        self.round_robin_index = 0\n        \n        logger.info(\"Load Balancer initialized\")\n    \n    def add_server(self, server_id: str, capacity: int = 100, weight: float = 1.0,\n                  metadata: Dict[str, Any] = None):\n        \"\"\"Add a server to the load balancer\"\"\"\n        \n        self.servers[server_id] = {\n            'capacity': capacity,\n            'current_load': 0,\n            'weight': weight,\n            'metadata': metadata or {},\n            'added_at': datetime.now()\n        }\n        \n        self.weights[server_id] = weight\n        self.health_status[server_id] = True\n        \n        logger.info(f\"Added server: {server_id} (capacity: {capacity}, weight: {weight})\")\n    \n    def remove_server(self, server_id: str):\n        \"\"\"Remove a server from the load balancer\"\"\"\n        \n        if server_id in self.servers:\n            del self.servers[server_id]\n            del self.request_counts[server_id]\n            del self.response_times[server_id]\n            del self.health_status[server_id]\n            del self.weights[server_id]\n            \n            logger.info(f\"Removed server: {server_id}\")\n    \n    def select_server(self, client_ip: str = None, request_metadata: Dict[str, Any] = None) -> Optional[str]:\n        \"\"\"Select optimal server based on current algorithm\"\"\"\n        \n        # Filter healthy servers\n        healthy_servers = [sid for sid, healthy in self.health_status.items() if healthy]\n        \n        if not healthy_servers:\n            logger.warning(\"No healthy servers available\")\n            return None\n        \n        # Use selected algorithm\n        algorithm_func = self.algorithms.get(self.current_algorithm, self._adaptive_selection)\n        selected_server = algorithm_func(healthy_servers, client_ip, request_metadata)\n        \n        if selected_server:\n            self.request_counts[selected_server] += 1\n            self.servers[selected_server]['current_load'] += 1\n        \n        return selected_server\n    \n    def record_response_time(self, server_id: str, response_time: float):\n        \"\"\"Record response time for a server\"\"\"\n        \n        self.response_times[server_id].append(response_time)\n        \n        # Keep only last 100 response times\n        if len(self.response_times[server_id]) > 100:\n            self.response_times[server_id] = self.response_times[server_id][-100:]\n        \n        # Update server load\n        if server_id in self.servers:\n            self.servers[server_id]['current_load'] = max(0, self.servers[server_id]['current_load'] - 1)\n    \n    def update_health_status(self, server_id: str, is_healthy: bool):\n        \"\"\"Update health status of a server\"\"\"\n        \n        self.health_status[server_id] = is_healthy\n        \n        if not is_healthy:\n            logger.warning(f\"Server {server_id} marked as unhealthy\")\n        else:\n            logger.info(f\"Server {server_id} marked as healthy\")\n    \n    def _round_robin(self, servers: List[str], client_ip: str = None, \n                    request_metadata: Dict[str, Any] = None) -> str:\n        \"\"\"Round robin selection\"\"\"\n        \n        if not servers:\n            return None\n        \n        selected = servers[self.round_robin_index % len(servers)]\n        self.round_robin_index += 1\n        \n        return selected\n    \n    def _weighted_round_robin(self, servers: List[str], client_ip: str = None,\n                            request_metadata: Dict[str, Any] = None) -> str:\n        \"\"\"Weighted round robin selection\"\"\"\n        \n        if not servers:\n            return None\n        \n        # Create weighted list\n        weighted_servers = []\n        for server in servers:\n            weight = int(self.weights[server] * 10)  # Scale weights\n            weighted_servers.extend([server] * weight)\n        \n        if not weighted_servers:\n            return servers[0]\n        \n        selected = weighted_servers[self.round_robin_index % len(weighted_servers)]\n        self.round_robin_index += 1\n        \n        return selected\n    \n    def _least_connections(self, servers: List[str], client_ip: str = None,\n                         request_metadata: Dict[str, Any] = None) -> str:\n        \"\"\"Least connections selection\"\"\"\n        \n        if not servers:\n            return None\n        \n        # Select server with least current load\n        min_load = float('inf')\n        selected_server = servers[0]\n        \n        for server in servers:\n            current_load = self.servers[server]['current_load']\n            if current_load < min_load:\n                min_load = current_load\n                selected_server = server\n        \n        return selected_server\n    \n    def _least_response_time(self, servers: List[str], client_ip: str = None,\n                           request_metadata: Dict[str, Any] = None) -> str:\n        \"\"\"Least response time selection\"\"\"\n        \n        if not servers:\n            return None\n        \n        min_response_time = float('inf')\n        selected_server = servers[0]\n        \n        for server in servers:\n            response_times = self.response_times[server]\n            if response_times:\n                avg_response_time = np.mean(response_times)\n                if avg_response_time < min_response_time:\n                    min_response_time = avg_response_time\n                    selected_server = server\n        \n        return selected_server\n    \n    def _weighted_least_connections(self, servers: List[str], client_ip: str = None,\n                                  request_metadata: Dict[str, Any] = None) -> str:\n        \"\"\"Weighted least connections selection\"\"\"\n        \n        if not servers:\n            return None\n        \n        min_weighted_load = float('inf')\n        selected_server = servers[0]\n        \n        for server in servers:\n            current_load = self.servers[server]['current_load']\n            weight = self.weights[server]\n            weighted_load = current_load / weight if weight > 0 else float('inf')\n            \n            if weighted_load < min_weighted_load:\n                min_weighted_load = weighted_load\n                selected_server = server\n        \n        return selected_server\n    \n    def _ip_hash(self, servers: List[str], client_ip: str = None,\n               request_metadata: Dict[str, Any] = None) -> str:\n        \"\"\"IP hash-based selection for session affinity\"\"\"\n        \n        if not servers or not client_ip:\n            return self._round_robin(servers, client_ip, request_metadata)\n        \n        # Hash client IP to select server\n        ip_hash = hash(client_ip)\n        server_index = ip_hash % len(servers)\n        \n        return servers[server_index]\n    \n    def _adaptive_selection(self, servers: List[str], client_ip: str = None,\n                          request_metadata: Dict[str, Any] = None) -> str:\n        \"\"\"Adaptive selection based on multiple factors\"\"\"\n        \n        if not servers:\n            return None\n        \n        if len(servers) == 1:\n            return servers[0]\n        \n        # Score each server\n        server_scores = []\n        \n        for server in servers:\n            score = 0.0\n            \n            # Load factor (30% weight)\n            current_load = self.servers[server]['current_load']\n            capacity = self.servers[server]['capacity']\n            load_ratio = current_load / capacity if capacity > 0 else 1.0\n            load_score = 1.0 - min(1.0, load_ratio)\n            score += load_score * 0.3\n            \n            # Response time factor (40% weight)\n            response_times = self.response_times[server]\n            if response_times:\n                avg_response_time = np.mean(response_times)\n                # Normalize response time (assume max reasonable response time is 10s)\n                response_score = max(0.0, 1.0 - (avg_response_time / 10.0))\n                score += response_score * 0.4\n            else:\n                score += 0.2  # Default score for new servers\n            \n            # Weight factor (20% weight)\n            weight = self.weights[server]\n            weight_score = min(1.0, weight / 2.0)  # Normalize assuming max weight is 2.0\n            score += weight_score * 0.2\n            \n            # Health factor (10% weight)\n            health_score = 1.0 if self.health_status[server] else 0.0\n            score += health_score * 0.1\n            \n            server_scores.append((server, score))\n        \n        # Select server with highest score\n        server_scores.sort(key=lambda x: x[1], reverse=True)\n        return server_scores[0][0]\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get load balancer statistics\"\"\"\n        \n        total_requests = sum(self.request_counts.values())\n        \n        server_stats = {}\n        for server_id, server_info in self.servers.items():\n            response_times = self.response_times[server_id]\n            avg_response_time = np.mean(response_times) if response_times else 0.0\n            \n            server_stats[server_id] = {\n                'capacity': server_info['capacity'],\n                'current_load': server_info['current_load'],\n                'utilization': server_info['current_load'] / server_info['capacity'] if server_info['capacity'] > 0 else 0,\n                'weight': self.weights[server_id],\n                'healthy': self.health_status[server_id],\n                'request_count': self.request_counts[server_id],\n                'request_percentage': (self.request_counts[server_id] / total_requests * 100) if total_requests > 0 else 0,\n                'avg_response_time': avg_response_time\n            }\n        \n        return {\n            'total_servers': len(self.servers),\n            'healthy_servers': sum(1 for healthy in self.health_status.values() if healthy),\n            'total_requests': total_requests,\n            'current_algorithm': self.current_algorithm,\n            'server_statistics': server_stats\n        }\n\n\nclass AutoScaler:\n    \"\"\"Intelligent auto-scaling system\"\"\"\n    \n    def __init__(self, strategy: ScalingStrategy = ScalingStrategy.HYBRID):\n        self.strategy = strategy\n        self.metrics_history: List[PerformanceMetrics] = []\n        self.scaling_history: List[ScalingDecision] = []\n        \n        # Scaling parameters\n        self.min_instances = 1\n        self.max_instances = 20\n        self.target_cpu_utilization = 0.7\n        self.target_response_time = 2.0  # seconds\n        self.scale_up_threshold = 0.8\n        self.scale_down_threshold = 0.3\n        self.cooldown_period = 300  # 5 minutes\n        \n        # Predictive scaling\n        self.prediction_window = 3600  # 1 hour\n        self.prediction_model = None  # Would be a trained ML model\n        \n        logger.info(f\"Auto Scaler initialized with strategy: {strategy.value}\")\n    \n    def add_metrics(self, metrics: PerformanceMetrics):\n        \"\"\"Add performance metrics for scaling decisions\"\"\"\n        \n        self.metrics_history.append(metrics)\n        \n        # Keep only last 24 hours of metrics\n        cutoff_time = datetime.now() - timedelta(hours=24)\n        self.metrics_history = [\n            m for m in self.metrics_history if m.timestamp > cutoff_time\n        ]\n    \n    def should_scale(self, current_instances: int) -> Optional[ScalingDecision]:\n        \"\"\"Determine if scaling is needed\"\"\"\n        \n        if not self.metrics_history:\n            return None\n        \n        # Check cooldown period\n        if self.scaling_history:\n            last_scaling = self.scaling_history[-1]\n            time_since_last_scaling = (datetime.now() - last_scaling.timestamp).total_seconds()\n            if time_since_last_scaling < self.cooldown_period:\n                return None\n        \n        # Get recent metrics\n        recent_metrics = self._get_recent_metrics(300)  # Last 5 minutes\n        if not recent_metrics:\n            return None\n        \n        # Apply scaling strategy\n        if self.strategy == ScalingStrategy.REACTIVE:\n            return self._reactive_scaling(current_instances, recent_metrics)\n        elif self.strategy == ScalingStrategy.PREDICTIVE:\n            return self._predictive_scaling(current_instances, recent_metrics)\n        elif self.strategy == ScalingStrategy.HYBRID:\n            return self._hybrid_scaling(current_instances, recent_metrics)\n        elif self.strategy == ScalingStrategy.COST_AWARE:\n            return self._cost_aware_scaling(current_instances, recent_metrics)\n        \n        return None\n    \n    def _reactive_scaling(self, current_instances: int, \n                        recent_metrics: List[PerformanceMetrics]) -> Optional[ScalingDecision]:\n        \"\"\"Reactive scaling based on current metrics\"\"\"\n        \n        # Calculate average metrics\n        avg_cpu = np.mean([m.cpu_usage for m in recent_metrics])\n        avg_response_time = np.mean([m.response_time for m in recent_metrics])\n        avg_queue_length = np.mean([m.queue_length for m in recent_metrics])\n        \n        # Determine scaling need\n        scale_up_signals = 0\n        scale_down_signals = 0\n        \n        # CPU utilization\n        if avg_cpu > self.scale_up_threshold:\n            scale_up_signals += 1\n        elif avg_cpu < self.scale_down_threshold:\n            scale_down_signals += 1\n        \n        # Response time\n        if avg_response_time > self.target_response_time * 1.5:\n            scale_up_signals += 1\n        elif avg_response_time < self.target_response_time * 0.5:\n            scale_down_signals += 1\n        \n        # Queue length\n        if avg_queue_length > 10:\n            scale_up_signals += 1\n        elif avg_queue_length < 2:\n            scale_down_signals += 1\n        \n        # Make scaling decision\n        if scale_up_signals >= 2 and current_instances < self.max_instances:\n            target_instances = min(self.max_instances, current_instances + 1)\n            confidence = min(1.0, scale_up_signals / 3.0)\n            \n            return ScalingDecision(\n                timestamp=datetime.now(),\n                current_instances=current_instances,\n                target_instances=target_instances,\n                scaling_reason=f\"Reactive scale up: CPU={avg_cpu:.2f}, RT={avg_response_time:.2f}s, Queue={avg_queue_length:.1f}\",\n                confidence=confidence,\n                estimated_cost_impact=self._estimate_cost_impact(current_instances, target_instances),\n                estimated_performance_impact=0.2,  # Expected 20% improvement\n                metadata={\n                    'avg_cpu': avg_cpu,\n                    'avg_response_time': avg_response_time,\n                    'avg_queue_length': avg_queue_length,\n                    'scale_up_signals': scale_up_signals\n                }\n            )\n        \n        elif scale_down_signals >= 2 and current_instances > self.min_instances:\n            target_instances = max(self.min_instances, current_instances - 1)\n            confidence = min(1.0, scale_down_signals / 3.0)\n            \n            return ScalingDecision(\n                timestamp=datetime.now(),\n                current_instances=current_instances,\n                target_instances=target_instances,\n                scaling_reason=f\"Reactive scale down: CPU={avg_cpu:.2f}, RT={avg_response_time:.2f}s, Queue={avg_queue_length:.1f}\",\n                confidence=confidence,\n                estimated_cost_impact=self._estimate_cost_impact(current_instances, target_instances),\n                estimated_performance_impact=-0.1,  # Slight performance reduction acceptable\n                metadata={\n                    'avg_cpu': avg_cpu,\n                    'avg_response_time': avg_response_time,\n                    'avg_queue_length': avg_queue_length,\n                    'scale_down_signals': scale_down_signals\n                }\n            )\n        \n        return None\n    \n    def _predictive_scaling(self, current_instances: int,\n                          recent_metrics: List[PerformanceMetrics]) -> Optional[ScalingDecision]:\n        \"\"\"Predictive scaling based on forecasted load\"\"\"\n        \n        # This is a simplified predictive model\n        # In practice, you'd use sophisticated ML models\n        \n        if len(self.metrics_history) < 100:  # Need sufficient history\n            return self._reactive_scaling(current_instances, recent_metrics)\n        \n        # Simple trend analysis\n        recent_cpu = [m.cpu_usage for m in self.metrics_history[-60:]]  # Last hour\n        cpu_trend = np.polyfit(range(len(recent_cpu)), recent_cpu, 1)[0]  # Linear trend\n        \n        recent_throughput = [m.throughput for m in self.metrics_history[-60:]]\n        throughput_trend = np.polyfit(range(len(recent_throughput)), recent_throughput, 1)[0]\n        \n        # Predict future load\n        current_cpu = recent_metrics[-1].cpu_usage\n        predicted_cpu = current_cpu + (cpu_trend * 12)  # 12 periods ahead (1 hour if 5-min intervals)\n        \n        current_throughput = recent_metrics[-1].throughput\n        predicted_throughput = current_throughput + (throughput_trend * 12)\n        \n        # Make scaling decision based on predictions\n        if predicted_cpu > self.scale_up_threshold and current_instances < self.max_instances:\n            # Calculate how many instances we might need\n            load_increase_ratio = predicted_cpu / current_cpu if current_cpu > 0 else 1.5\n            suggested_instances = int(current_instances * load_increase_ratio)\n            target_instances = min(self.max_instances, suggested_instances)\n            \n            confidence = min(1.0, abs(cpu_trend) * 10)  # Higher confidence for stronger trends\n            \n            return ScalingDecision(\n                timestamp=datetime.now(),\n                current_instances=current_instances,\n                target_instances=target_instances,\n                scaling_reason=f\"Predictive scale up: Current CPU={current_cpu:.2f}, Predicted={predicted_cpu:.2f}, Trend={cpu_trend:.4f}\",\n                confidence=confidence,\n                estimated_cost_impact=self._estimate_cost_impact(current_instances, target_instances),\n                estimated_performance_impact=0.3,\n                metadata={\n                    'current_cpu': current_cpu,\n                    'predicted_cpu': predicted_cpu,\n                    'cpu_trend': cpu_trend,\n                    'current_throughput': current_throughput,\n                    'predicted_throughput': predicted_throughput,\n                    'throughput_trend': throughput_trend\n                }\n            )\n        \n        elif predicted_cpu < self.scale_down_threshold and current_instances > self.min_instances:\n            target_instances = max(self.min_instances, current_instances - 1)\n            confidence = min(1.0, abs(cpu_trend) * 10)\n            \n            return ScalingDecision(\n                timestamp=datetime.now(),\n                current_instances=current_instances,\n                target_instances=target_instances,\n                scaling_reason=f\"Predictive scale down: Current CPU={current_cpu:.2f}, Predicted={predicted_cpu:.2f}, Trend={cpu_trend:.4f}\",\n                confidence=confidence,\n                estimated_cost_impact=self._estimate_cost_impact(current_instances, target_instances),\n                estimated_performance_impact=-0.05,\n                metadata={\n                    'current_cpu': current_cpu,\n                    'predicted_cpu': predicted_cpu,\n                    'cpu_trend': cpu_trend\n                }\n            )\n        \n        return None\n"    
\n    def _hybrid_scaling(self, current_instances: int,\n                       recent_metrics: List[PerformanceMetrics]) -> Optional[ScalingDecision]:\n        \"\"\"Hybrid scaling combining reactive and predictive approaches\"\"\"\n        \n        # Get both reactive and predictive decisions\n        reactive_decision = self._reactive_scaling(current_instances, recent_metrics)\n        predictive_decision = self._predictive_scaling(current_instances, recent_metrics)\n        \n        # If both agree, use the decision with higher confidence\n        if reactive_decision and predictive_decision:\n            if (reactive_decision.target_instances > current_instances and \n                predictive_decision.target_instances > current_instances):\n                # Both suggest scaling up\n                if reactive_decision.confidence > predictive_decision.confidence:\n                    reactive_decision.scaling_reason = f\"Hybrid (reactive): {reactive_decision.scaling_reason}\"\n                    return reactive_decision\n                else:\n                    predictive_decision.scaling_reason = f\"Hybrid (predictive): {predictive_decision.scaling_reason}\"\n                    return predictive_decision\n            \n            elif (reactive_decision.target_instances < current_instances and \n                  predictive_decision.target_instances < current_instances):\n                # Both suggest scaling down\n                if reactive_decision.confidence > predictive_decision.confidence:\n                    reactive_decision.scaling_reason = f\"Hybrid (reactive): {reactive_decision.scaling_reason}\"\n                    return reactive_decision\n                else:\n                    predictive_decision.scaling_reason = f\"Hybrid (predictive): {predictive_decision.scaling_reason}\"\n                    return predictive_decision\n        \n        # If only one suggests scaling, use it if confidence is high enough\n        if reactive_decision and reactive_decision.confidence > 0.7:\n            reactive_decision.scaling_reason = f\"Hybrid (reactive only): {reactive_decision.scaling_reason}\"\n            return reactive_decision\n        \n        if predictive_decision and predictive_decision.confidence > 0.7:\n            predictive_decision.scaling_reason = f\"Hybrid (predictive only): {predictive_decision.scaling_reason}\"\n            return predictive_decision\n        \n        return None\n    \n    def _cost_aware_scaling(self, current_instances: int,\n                          recent_metrics: List[PerformanceMetrics]) -> Optional[ScalingDecision]:\n        \"\"\"Cost-aware scaling that considers cost implications\"\"\"\n        \n        # Get base scaling decision\n        base_decision = self._hybrid_scaling(current_instances, recent_metrics)\n        \n        if not base_decision:\n            return None\n        \n        # Calculate cost implications\n        cost_impact = self._estimate_cost_impact(current_instances, base_decision.target_instances)\n        \n        # Get current cost per request\n        avg_cost_per_request = np.mean([m.cost_per_request for m in recent_metrics])\n        \n        # Adjust decision based on cost considerations\n        if base_decision.target_instances > current_instances:\n            # Scaling up - check if cost increase is justified\n            if cost_impact > 100:  # More than $100 increase\n                # Only scale up if performance is severely degraded\n                avg_response_time = np.mean([m.response_time for m in recent_metrics])\n                if avg_response_time < self.target_response_time * 2:\n                    # Performance not bad enough to justify high cost\n                    return None\n            \n            # Reduce scaling if cost impact is high\n            if cost_impact > 50:\n                base_decision.target_instances = min(\n                    base_decision.target_instances,\n                    current_instances + 1  # Scale more conservatively\n                )\n        \n        elif base_decision.target_instances < current_instances:\n            # Scaling down - more aggressive if cost savings are significant\n            if cost_impact < -20:  # Significant cost savings\n                base_decision.confidence = min(1.0, base_decision.confidence + 0.2)\n        \n        base_decision.scaling_reason = f\"Cost-aware: {base_decision.scaling_reason} (Cost impact: ${cost_impact:.2f})\"\n        base_decision.estimated_cost_impact = cost_impact\n        \n        return base_decision\n    \n    def _get_recent_metrics(self, seconds: int) -> List[PerformanceMetrics]:\n        \"\"\"Get metrics from the last N seconds\"\"\"\n        \n        cutoff_time = datetime.now() - timedelta(seconds=seconds)\n        return [m for m in self.metrics_history if m.timestamp > cutoff_time]\n    \n    def _estimate_cost_impact(self, current_instances: int, target_instances: int) -> float:\n        \"\"\"Estimate cost impact of scaling decision\"\"\"\n        \n        # Simplified cost calculation\n        cost_per_instance_per_hour = 0.50  # $0.50 per hour per instance\n        instance_diff = target_instances - current_instances\n        \n        # Estimate hourly cost impact\n        hourly_cost_impact = instance_diff * cost_per_instance_per_hour\n        \n        return hourly_cost_impact\n    \n    def execute_scaling(self, decision: ScalingDecision) -> bool:\n        \"\"\"Execute scaling decision\"\"\"\n        \n        # Record the scaling decision\n        self.scaling_history.append(decision)\n        \n        # In a real implementation, this would trigger actual scaling\n        # For now, we just log the decision\n        logger.info(f\"Executing scaling: {decision.current_instances} -> {decision.target_instances}\")\n        logger.info(f\"Reason: {decision.scaling_reason}\")\n        logger.info(f\"Confidence: {decision.confidence:.3f}\")\n        logger.info(f\"Cost Impact: ${decision.estimated_cost_impact:.2f}/hour\")\n        \n        return True\n    \n    def get_scaling_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get auto-scaling statistics\"\"\"\n        \n        if not self.scaling_history:\n            return {\n                'total_scaling_events': 0,\n                'scale_up_events': 0,\n                'scale_down_events': 0,\n                'average_confidence': 0.0,\n                'total_cost_impact': 0.0\n            }\n        \n        scale_up_events = sum(1 for d in self.scaling_history if d.target_instances > d.current_instances)\n        scale_down_events = sum(1 for d in self.scaling_history if d.target_instances < d.current_instances)\n        \n        avg_confidence = np.mean([d.confidence for d in self.scaling_history])\n        total_cost_impact = sum(d.estimated_cost_impact for d in self.scaling_history)\n        \n        return {\n            'total_scaling_events': len(self.scaling_history),\n            'scale_up_events': scale_up_events,\n            'scale_down_events': scale_down_events,\n            'average_confidence': avg_confidence,\n            'total_cost_impact': total_cost_impact,\n            'strategy': self.strategy.value,\n            'current_parameters': {\n                'min_instances': self.min_instances,\n                'max_instances': self.max_instances,\n                'target_cpu_utilization': self.target_cpu_utilization,\n                'cooldown_period': self.cooldown_period\n            }\n        }\n\n\nclass PerformanceOptimizer:\n    \"\"\"Main performance optimization coordinator\"\"\"\n    \n    def __init__(self, optimization_objective: OptimizationObjective = OptimizationObjective.BALANCE_ALL):\n        self.optimization_objective = optimization_objective\n        \n        # Initialize components\n        self.cache = IntelligentCache()\n        self.load_balancer = LoadBalancer()\n        self.auto_scaler = AutoScaler()\n        \n        # Performance monitoring\n        self.metrics_history: List[PerformanceMetrics] = []\n        self.optimization_history: List[Dict[str, Any]] = []\n        \n        # Resource monitoring\n        self.resource_monitor = ResourceMonitor()\n        \n        # Optimization parameters\n        self.optimization_params = {\n            'cache_hit_rate_target': 0.8,\n            'response_time_target': 2.0,\n            'cpu_utilization_target': 0.7,\n            'cost_per_request_target': 0.01,\n            'throughput_target': 100.0\n        }\n        \n        logger.info(f\"Performance Optimizer initialized with objective: {optimization_objective.value}\")\n    \n    async def optimize_performance(self, current_metrics: PerformanceMetrics) -> Dict[str, Any]:\n        \"\"\"Main optimization function\"\"\"\n        \n        optimization_start = datetime.now()\n        \n        # Add metrics to history\n        self.metrics_history.append(current_metrics)\n        self.auto_scaler.add_metrics(current_metrics)\n        \n        # Perform optimizations based on objective\n        optimizations = {\n            'cache_optimizations': [],\n            'load_balancing_optimizations': [],\n            'scaling_optimizations': [],\n            'resource_optimizations': []\n        }\n        \n        # Cache optimization\n        cache_opts = await self._optimize_cache(current_metrics)\n        optimizations['cache_optimizations'] = cache_opts\n        \n        # Load balancing optimization\n        lb_opts = self._optimize_load_balancing(current_metrics)\n        optimizations['load_balancing_optimizations'] = lb_opts\n        \n        # Auto-scaling optimization\n        scaling_opts = self._optimize_scaling(current_metrics)\n        optimizations['scaling_optimizations'] = scaling_opts\n        \n        # Resource optimization\n        resource_opts = self._optimize_resources(current_metrics)\n        optimizations['resource_optimizations'] = resource_opts\n        \n        # Calculate optimization impact\n        optimization_impact = self._calculate_optimization_impact(optimizations, current_metrics)\n        \n        # Record optimization\n        optimization_record = {\n            'timestamp': optimization_start,\n            'objective': self.optimization_objective.value,\n            'current_metrics': asdict(current_metrics),\n            'optimizations': optimizations,\n            'optimization_impact': optimization_impact,\n            'optimization_time': (datetime.now() - optimization_start).total_seconds()\n        }\n        \n        self.optimization_history.append(optimization_record)\n        \n        logger.info(f\"Performance optimization completed in {optimization_record['optimization_time']:.3f}s\")\n        \n        return optimization_record\n    \n    async def _optimize_cache(self, metrics: PerformanceMetrics) -> List[Dict[str, Any]]:\n        \"\"\"Optimize caching strategy\"\"\"\n        \n        optimizations = []\n        cache_stats = self.cache.get_statistics()\n        \n        # Check cache hit rate\n        if cache_stats['hit_rate'] < self.optimization_params['cache_hit_rate_target']:\n            # Suggest cache strategy change\n            if cache_stats['hit_rate'] < 0.5:\n                # Very low hit rate - suggest semantic caching\n                optimizations.append({\n                    'type': 'cache_strategy_change',\n                    'current_strategy': cache_stats['strategy'],\n                    'suggested_strategy': 'semantic',\n                    'reason': f\"Low hit rate ({cache_stats['hit_rate']:.2f}) suggests need for semantic caching\",\n                    'expected_improvement': 0.3\n                })\n            else:\n                # Moderate hit rate - suggest adaptive caching\n                optimizations.append({\n                    'type': 'cache_strategy_change',\n                    'current_strategy': cache_stats['strategy'],\n                    'suggested_strategy': 'adaptive',\n                    'reason': f\"Moderate hit rate ({cache_stats['hit_rate']:.2f}) could benefit from adaptive strategy\",\n                    'expected_improvement': 0.15\n                })\n        \n        # Check cache utilization\n        if cache_stats['utilization'] > 0.9:\n            # Cache is nearly full - suggest size increase\n            optimizations.append({\n                'type': 'cache_size_increase',\n                'current_size': cache_stats['max_size'],\n                'suggested_size': int(cache_stats['max_size'] * 1.5),\n                'reason': f\"High cache utilization ({cache_stats['utilization']:.2f}) suggests need for more space\",\n                'expected_improvement': 0.1\n            })\n        \n        # Check for cache warming opportunities\n        if metrics.response_time > self.optimization_params['response_time_target']:\n            optimizations.append({\n                'type': 'cache_warming',\n                'reason': f\"High response time ({metrics.response_time:.2f}s) suggests need for cache warming\",\n                'suggested_warming_strategy': 'predictive_preloading',\n                'expected_improvement': 0.2\n            })\n        \n        return optimizations\n    \n    def _optimize_load_balancing(self, metrics: PerformanceMetrics) -> List[Dict[str, Any]]:\n        \"\"\"Optimize load balancing strategy\"\"\"\n        \n        optimizations = []\n        lb_stats = self.load_balancer.get_statistics()\n        \n        # Check for uneven load distribution\n        if lb_stats['server_statistics']:\n            utilizations = [s['utilization'] for s in lb_stats['server_statistics'].values()]\n            utilization_std = np.std(utilizations)\n            \n            if utilization_std > 0.2:  # High variance in utilization\n                optimizations.append({\n                    'type': 'load_balancing_algorithm_change',\n                    'current_algorithm': lb_stats['current_algorithm'],\n                    'suggested_algorithm': 'weighted_least_connections',\n                    'reason': f\"High utilization variance ({utilization_std:.3f}) suggests uneven load distribution\",\n                    'expected_improvement': 0.15\n                })\n        \n        # Check for unhealthy servers\n        unhealthy_servers = lb_stats['total_servers'] - lb_stats['healthy_servers']\n        if unhealthy_servers > 0:\n            optimizations.append({\n                'type': 'server_health_check',\n                'unhealthy_servers': unhealthy_servers,\n                'reason': f\"{unhealthy_servers} unhealthy servers detected\",\n                'suggested_action': 'investigate_and_recover',\n                'expected_improvement': 0.1\n            })\n        \n        return optimizations\n    \n    def _optimize_scaling(self, metrics: PerformanceMetrics) -> List[Dict[str, Any]]:\n        \"\"\"Optimize auto-scaling configuration\"\"\"\n        \n        optimizations = []\n        \n        # Get current instance count (simulated)\n        current_instances = 3  # This would come from actual infrastructure\n        \n        # Check if scaling is needed\n        scaling_decision = self.auto_scaler.should_scale(current_instances)\n        \n        if scaling_decision:\n            optimizations.append({\n                'type': 'scaling_recommendation',\n                'current_instances': scaling_decision.current_instances,\n                'target_instances': scaling_decision.target_instances,\n                'reason': scaling_decision.scaling_reason,\n                'confidence': scaling_decision.confidence,\n                'cost_impact': scaling_decision.estimated_cost_impact,\n                'performance_impact': scaling_decision.estimated_performance_impact\n            })\n        \n        # Check scaling parameters\n        scaling_stats = self.auto_scaler.get_scaling_statistics()\n        \n        if scaling_stats['total_scaling_events'] > 0:\n            # Analyze scaling frequency\n            recent_scalings = [d for d in self.auto_scaler.scaling_history \n                             if (datetime.now() - d.timestamp).total_seconds() < 3600]  # Last hour\n            \n            if len(recent_scalings) > 5:  # Too frequent scaling\n                optimizations.append({\n                    'type': 'scaling_parameter_adjustment',\n                    'parameter': 'cooldown_period',\n                    'current_value': self.auto_scaler.cooldown_period,\n                    'suggested_value': self.auto_scaler.cooldown_period * 1.5,\n                    'reason': f\"Frequent scaling ({len(recent_scalings)} events in last hour) suggests need for longer cooldown\",\n                    'expected_improvement': 0.1\n                })\n        \n        return optimizations\n    \n    def _optimize_resources(self, metrics: PerformanceMetrics) -> List[Dict[str, Any]]:\n        \"\"\"Optimize resource allocation\"\"\"\n        \n        optimizations = []\n        \n        # CPU optimization\n        if metrics.cpu_usage > 0.9:\n            optimizations.append({\n                'type': 'cpu_optimization',\n                'current_usage': metrics.cpu_usage,\n                'reason': 'High CPU usage detected',\n                'suggested_actions': [\n                    'Enable CPU-intensive task queuing',\n                    'Implement request throttling',\n                    'Consider CPU-optimized instance types'\n                ],\n                'expected_improvement': 0.2\n            })\n        \n        # Memory optimization\n        if metrics.memory_usage > 0.85:\n            optimizations.append({\n                'type': 'memory_optimization',\n                'current_usage': metrics.memory_usage,\n                'reason': 'High memory usage detected',\n                'suggested_actions': [\n                    'Implement memory-efficient data structures',\n                    'Enable garbage collection tuning',\n                    'Consider memory-optimized instance types'\n                ],\n                'expected_improvement': 0.15\n            })\n        \n        # Queue optimization\n        if metrics.queue_length > 50:\n            optimizations.append({\n                'type': 'queue_optimization',\n                'current_queue_length': metrics.queue_length,\n                'reason': 'High queue length detected',\n                'suggested_actions': [\n                    'Implement priority queuing',\n                    'Enable request batching',\n                    'Consider additional worker processes'\n                ],\n                'expected_improvement': 0.25\n            })\n        \n        return optimizations\n    \n    def _calculate_optimization_impact(self, optimizations: Dict[str, List[Dict[str, Any]]],\n                                     current_metrics: PerformanceMetrics) -> Dict[str, float]:\n        \"\"\"Calculate expected impact of optimizations\"\"\"\n        \n        impact = {\n            'response_time_improvement': 0.0,\n            'throughput_improvement': 0.0,\n            'cost_reduction': 0.0,\n            'reliability_improvement': 0.0,\n            'overall_score': 0.0\n        }\n        \n        # Calculate cumulative improvements\n        for category, opts in optimizations.items():\n            for opt in opts:\n                expected_improvement = opt.get('expected_improvement', 0.0)\n                \n                if 'cache' in category:\n                    impact['response_time_improvement'] += expected_improvement * 0.3\n                    impact['throughput_improvement'] += expected_improvement * 0.2\n                elif 'load_balancing' in category:\n                    impact['response_time_improvement'] += expected_improvement * 0.2\n                    impact['reliability_improvement'] += expected_improvement * 0.4\n                elif 'scaling' in category:\n                    impact['throughput_improvement'] += expected_improvement * 0.4\n                    if opt.get('cost_impact', 0) < 0:  # Cost reduction\n                        impact['cost_reduction'] += abs(opt['cost_impact']) / 100  # Normalize\n                elif 'resource' in category:\n                    impact['response_time_improvement'] += expected_improvement * 0.25\n                    impact['reliability_improvement'] += expected_improvement * 0.3\n        \n        # Calculate overall score based on optimization objective\n        if self.optimization_objective == OptimizationObjective.MINIMIZE_LATENCY:\n            impact['overall_score'] = impact['response_time_improvement'] * 0.8 + impact['reliability_improvement'] * 0.2\n        elif self.optimization_objective == OptimizationObjective.MINIMIZE_COST:\n            impact['overall_score'] = impact['cost_reduction'] * 0.6 + impact['response_time_improvement'] * 0.4\n        elif self.optimization_objective == OptimizationObjective.MAXIMIZE_THROUGHPUT:\n            impact['overall_score'] = impact['throughput_improvement'] * 0.7 + impact['response_time_improvement'] * 0.3\n        elif self.optimization_objective == OptimizationObjective.MAXIMIZE_ACCURACY:\n            impact['overall_score'] = impact['reliability_improvement'] * 0.6 + impact['response_time_improvement'] * 0.4\n        else:  # BALANCE_ALL\n            impact['overall_score'] = (\n                impact['response_time_improvement'] * 0.3 +\n                impact['throughput_improvement'] * 0.25 +\n                impact['cost_reduction'] * 0.25 +\n                impact['reliability_improvement'] * 0.2\n            )\n        \n        return impact\n    \n    def get_optimization_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get optimization statistics\"\"\"\n        \n        if not self.optimization_history:\n            return {\n                'total_optimizations': 0,\n                'average_optimization_time': 0.0,\n                'average_impact_score': 0.0\n            }\n        \n        total_optimizations = len(self.optimization_history)\n        avg_optimization_time = np.mean([o['optimization_time'] for o in self.optimization_history])\n        avg_impact_score = np.mean([o['optimization_impact']['overall_score'] for o in self.optimization_history])\n        \n        # Count optimization types\n        optimization_types = defaultdict(int)\n        for record in self.optimization_history:\n            for category, opts in record['optimizations'].items():\n                optimization_types[category] += len(opts)\n        \n        return {\n            'total_optimizations': total_optimizations,\n            'average_optimization_time': avg_optimization_time,\n            'average_impact_score': avg_impact_score,\n            'optimization_objective': self.optimization_objective.value,\n            'optimization_types': dict(optimization_types),\n            'cache_statistics': self.cache.get_statistics(),\n            'load_balancer_statistics': self.load_balancer.get_statistics(),\n            'auto_scaler_statistics': self.auto_scaler.get_scaling_statistics()\n        }\n\n\nclass ResourceMonitor:\n    \"\"\"System resource monitoring\"\"\"\n    \n    def __init__(self):\n        self.monitoring_active = True\n        self.resource_history: List[Dict[str, Any]] = []\n        \n        # Start monitoring thread\n        self.monitor_thread = threading.Thread(target=self._monitor_resources, daemon=True)\n        self.monitor_thread.start()\n        \n        logger.info(\"Resource Monitor initialized\")\n    \n    def _monitor_resources(self):\n        \"\"\"Background resource monitoring\"\"\"\n        \n        while self.monitoring_active:\n            try:\n                # Simulate resource monitoring\n                # In practice, this would use psutil or similar\n                \n                resource_data = {\n                    'timestamp': datetime.now(),\n                    'cpu_usage': np.random.uniform(0.3, 0.9),\n                    'memory_usage': np.random.uniform(0.4, 0.8),\n                    'disk_usage': np.random.uniform(0.2, 0.7),\n                    'network_io': np.random.uniform(100, 1000),  # MB/s\n                    'disk_io': np.random.uniform(50, 500),  # MB/s\n                    'active_connections': np.random.randint(10, 100),\n                    'thread_count': np.random.randint(20, 200)\n                }\n                \n                self.resource_history.append(resource_data)\n                \n                # Keep only last 24 hours\n                cutoff_time = datetime.now() - timedelta(hours=24)\n                self.resource_history = [\n                    r for r in self.resource_history if r['timestamp'] > cutoff_time\n                ]\n                \n                time.sleep(60)  # Monitor every minute\n                \n            except Exception as e:\n                logger.error(f\"Resource monitoring error: {e}\")\n                time.sleep(60)\n    \n    def get_current_resources(self) -> Dict[str, float]:\n        \"\"\"Get current resource utilization\"\"\"\n        \n        if self.resource_history:\n            return self.resource_history[-1]\n        \n        return {\n            'cpu_usage': 0.5,\n            'memory_usage': 0.5,\n            'disk_usage': 0.3,\n            'network_io': 200,\n            'disk_io': 100,\n            'active_connections': 50,\n            'thread_count': 100\n        }\n    \n    def get_resource_trends(self, hours: int = 1) -> Dict[str, float]:\n        \"\"\"Get resource usage trends\"\"\"\n        \n        cutoff_time = datetime.now() - timedelta(hours=hours)\n        recent_data = [r for r in self.resource_history if r['timestamp'] > cutoff_time]\n        \n        if len(recent_data) < 2:\n            return {}\n        \n        trends = {}\n        \n        for metric in ['cpu_usage', 'memory_usage', 'disk_usage', 'network_io', 'disk_io']:\n            values = [r[metric] for r in recent_data]\n            if len(values) > 1:\n                # Calculate linear trend\n                trend = np.polyfit(range(len(values)), values, 1)[0]\n                trends[f\"{metric}_trend\"] = trend\n        \n        return trends\n\n\n# Factory functions\ndef create_performance_optimizer(objective: OptimizationObjective = OptimizationObjective.BALANCE_ALL) -> PerformanceOptimizer:\n    \"\"\"Factory function to create performance optimizer\"\"\"\n    return PerformanceOptimizer(objective)\n\n\ndef create_intelligent_cache(strategy: CacheStrategy = CacheStrategy.ADAPTIVE) -> IntelligentCache:\n    \"\"\"Factory function to create intelligent cache\"\"\"\n    return IntelligentCache(strategy=strategy)\n\n\ndef create_auto_scaler(strategy: ScalingStrategy = ScalingStrategy.HYBRID) -> AutoScaler:\n    \"\"\"Factory function to create auto scaler\"\"\"\n    return AutoScaler(strategy)\n