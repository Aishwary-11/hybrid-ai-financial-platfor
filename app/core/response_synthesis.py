#!/usr/bin/env python3\n\"\"\"\nResponse Synthesis Algorithms\nAdvanced multi-model output combination and conflict resolution\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom typing import Dict, List, Optional, Any, Tuple, Union\nfrom datetime import datetime\nimport json\nimport logging\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nfrom abc import ABC, abstractmethod\nimport hashlib\nfrom collections import defaultdict, Counter\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom app.core.hybrid_ai_engine import ModelOutput, TaskCategory\n\nlogger = logging.getLogger(__name__)\n\n\nclass SynthesisStrategy(Enum):\n    \"\"\"Response synthesis strategies\"\"\"\n    WEIGHTED_AVERAGE = \"weighted_average\"\n    CONFIDENCE_WEIGHTED = \"confidence_weighted\"\n    MAJORITY_VOTING = \"majority_voting\"\n    CONSENSUS_BUILDING = \"consensus_building\"\n    HIERARCHICAL_FUSION = \"hierarchical_fusion\"\n    SEMANTIC_MERGING = \"semantic_merging\"\n    CONTRADICTION_RESOLUTION = \"contradiction_resolution\"\n    NARRATIVE_SYNTHESIS = \"narrative_synthesis\"\n\n\nclass ConflictType(Enum):\n    \"\"\"Types of conflicts between model outputs\"\"\"\n    NUMERICAL_DISAGREEMENT = \"numerical_disagreement\"\n    CATEGORICAL_CONFLICT = \"categorical_conflict\"\n    CONFIDENCE_MISMATCH = \"confidence_mismatch\"\n    SEMANTIC_CONTRADICTION = \"semantic_contradiction\"\n    TEMPORAL_INCONSISTENCY = \"temporal_inconsistency\"\n    LOGICAL_INCONSISTENCY = \"logical_inconsistency\"\n\n\nclass UncertaintyType(Enum):\n    \"\"\"Types of uncertainty in responses\"\"\"\n    ALEATORY = \"aleatory\"  # Inherent randomness\n    EPISTEMIC = \"epistemic\"  # Knowledge uncertainty\n    MODEL_UNCERTAINTY = \"model_uncertainty\"  # Model-specific uncertainty\n    DATA_UNCERTAINTY = \"data_uncertainty\"  # Input data uncertainty\n\n\n@dataclass\nclass SynthesisResult:\n    \"\"\"Result of response synthesis\"\"\"\n    synthesized_response: Dict[str, Any]\n    synthesis_strategy: SynthesisStrategy\n    confidence_score: float\n    uncertainty_quantification: Dict[str, float]\n    conflicts_detected: List[Dict[str, Any]]\n    conflicts_resolved: List[Dict[str, Any]]\n    individual_contributions: Dict[str, float]\n    synthesis_metadata: Dict[str, Any]\n    quality_metrics: Dict[str, float]\n    timestamp: datetime\n\n\n@dataclass\nclass ConflictDetection:\n    \"\"\"Detected conflict between model outputs\"\"\"\n    conflict_id: str\n    conflict_type: ConflictType\n    conflicting_models: List[str]\n    conflict_description: str\n    severity: float  # 0.0 to 1.0\n    affected_fields: List[str]\n    resolution_strategy: Optional[str]\n    confidence_impact: float\n    metadata: Dict[str, Any]\n\n\nclass ResponseSynthesizer:\n    \"\"\"Advanced response synthesis engine\"\"\"\n    \n    def __init__(self):\n        self.synthesis_strategies = {\n            SynthesisStrategy.WEIGHTED_AVERAGE: self._weighted_average_synthesis,\n            SynthesisStrategy.CONFIDENCE_WEIGHTED: self._confidence_weighted_synthesis,\n            SynthesisStrategy.MAJORITY_VOTING: self._majority_voting_synthesis,\n            SynthesisStrategy.CONSENSUS_BUILDING: self._consensus_building_synthesis,\n            SynthesisStrategy.HIERARCHICAL_FUSION: self._hierarchical_fusion_synthesis,\n            SynthesisStrategy.SEMANTIC_MERGING: self._semantic_merging_synthesis,\n            SynthesisStrategy.CONTRADICTION_RESOLUTION: self._contradiction_resolution_synthesis,\n            SynthesisStrategy.NARRATIVE_SYNTHESIS: self._narrative_synthesis\n        }\n        \n        self.conflict_detectors = {\n            ConflictType.NUMERICAL_DISAGREEMENT: self._detect_numerical_conflicts,\n            ConflictType.CATEGORICAL_CONFLICT: self._detect_categorical_conflicts,\n            ConflictType.CONFIDENCE_MISMATCH: self._detect_confidence_conflicts,\n            ConflictType.SEMANTIC_CONTRADICTION: self._detect_semantic_conflicts,\n            ConflictType.TEMPORAL_INCONSISTENCY: self._detect_temporal_conflicts,\n            ConflictType.LOGICAL_INCONSISTENCY: self._detect_logical_conflicts\n        }\n        \n        self.synthesis_history: List[SynthesisResult] = []\n        self.performance_metrics = defaultdict(list)\n        \n        # Initialize TF-IDF vectorizer for semantic analysis\n        self.tfidf_vectorizer = TfidfVectorizer(\n            max_features=1000,\n            stop_words='english',\n            ngram_range=(1, 2)\n        )\n        \n        logger.info(\"Response Synthesizer initialized\")\n    \n    def synthesize_responses(self, model_outputs: Dict[str, ModelOutput],\n                           strategy: SynthesisStrategy = SynthesisStrategy.CONFIDENCE_WEIGHTED,\n                           synthesis_params: Dict[str, Any] = None) -> SynthesisResult:\n        \"\"\"Synthesize multiple model outputs into a coherent response\"\"\"\n        \n        synthesis_params = synthesis_params or {}\n        start_time = datetime.now()\n        \n        logger.info(f\"Starting response synthesis with strategy: {strategy.value}\")\n        logger.info(f\"Synthesizing {len(model_outputs)} model outputs\")\n        \n        # Step 1: Detect conflicts\n        conflicts = self._detect_all_conflicts(model_outputs)\n        logger.info(f\"Detected {len(conflicts)} conflicts\")\n        \n        # Step 2: Apply synthesis strategy\n        synthesis_func = self.synthesis_strategies[strategy]\n        synthesized_response, individual_contributions = synthesis_func(\n            model_outputs, conflicts, synthesis_params\n        )\n        \n        # Step 3: Quantify uncertainty\n        uncertainty_quantification = self._quantify_uncertainty(\n            model_outputs, conflicts, synthesized_response\n        )\n        \n        # Step 4: Calculate confidence score\n        confidence_score = self._calculate_synthesis_confidence(\n            model_outputs, conflicts, individual_contributions\n        )\n        \n        # Step 5: Resolve conflicts\n        resolved_conflicts = self._resolve_conflicts(\n            conflicts, synthesized_response, strategy\n        )\n        \n        # Step 6: Calculate quality metrics\n        quality_metrics = self._calculate_quality_metrics(\n            model_outputs, synthesized_response, conflicts\n        )\n        \n        # Create synthesis result\n        result = SynthesisResult(\n            synthesized_response=synthesized_response,\n            synthesis_strategy=strategy,\n            confidence_score=confidence_score,\n            uncertainty_quantification=uncertainty_quantification,\n            conflicts_detected=conflicts,\n            conflicts_resolved=resolved_conflicts,\n            individual_contributions=individual_contributions,\n            synthesis_metadata={\n                \"num_models\": len(model_outputs),\n                \"synthesis_time\": (datetime.now() - start_time).total_seconds(),\n                \"strategy_params\": synthesis_params,\n                \"model_names\": list(model_outputs.keys())\n            },\n            quality_metrics=quality_metrics,\n            timestamp=datetime.now()\n        )\n        \n        # Store in history\n        self.synthesis_history.append(result)\n        \n        # Update performance metrics\n        self._update_performance_metrics(result)\n        \n        logger.info(f\"Synthesis completed with confidence: {confidence_score:.3f}\")\n        \n        return result\n    \n    def _detect_all_conflicts(self, model_outputs: Dict[str, ModelOutput]) -> List[ConflictDetection]:\n        \"\"\"Detect all types of conflicts between model outputs\"\"\"\n        \n        all_conflicts = []\n        \n        for conflict_type, detector_func in self.conflict_detectors.items():\n            conflicts = detector_func(model_outputs)\n            all_conflicts.extend(conflicts)\n        \n        # Sort conflicts by severity\n        all_conflicts.sort(key=lambda c: c.severity, reverse=True)\n        \n        return all_conflicts\n    \n    def _detect_numerical_conflicts(self, model_outputs: Dict[str, ModelOutput]) -> List[ConflictDetection]:\n        \"\"\"Detect numerical disagreements between models\"\"\"\n        \n        conflicts = []\n        \n        # Extract numerical values from all outputs\n        numerical_fields = defaultdict(dict)\n        \n        for model_name, output in model_outputs.items():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                for field, value in output.result.items():\n                    if isinstance(value, (int, float)):\n                        numerical_fields[field][model_name] = value\n        \n        # Check for disagreements\n        for field, model_values in numerical_fields.items():\n            if len(model_values) > 1:\n                values = list(model_values.values())\n                mean_value = np.mean(values)\n                std_value = np.std(values)\n                \n                # Consider it a conflict if standard deviation is high relative to mean\n                if std_value > 0 and (std_value / abs(mean_value)) > 0.2:\n                    conflict = ConflictDetection(\n                        conflict_id=f\"numerical_{field}_{hash(str(model_values)) % 10000}\",\n                        conflict_type=ConflictType.NUMERICAL_DISAGREEMENT,\n                        conflicting_models=list(model_values.keys()),\n                        conflict_description=f\"Numerical disagreement in field '{field}': {model_values}\",\n                        severity=min(1.0, std_value / abs(mean_value)),\n                        affected_fields=[field],\n                        resolution_strategy=\"weighted_average\",\n                        confidence_impact=0.1 * (std_value / abs(mean_value)),\n                        metadata={\n                            \"values\": model_values,\n                            \"mean\": mean_value,\n                            \"std\": std_value,\n                            \"coefficient_of_variation\": std_value / abs(mean_value)\n                        }\n                    )\n                    conflicts.append(conflict)\n        \n        return conflicts\n    \n    def _detect_categorical_conflicts(self, model_outputs: Dict[str, ModelOutput]) -> List[ConflictDetection]:\n        \"\"\"Detect categorical conflicts between models\"\"\"\n        \n        conflicts = []\n        \n        # Extract categorical values\n        categorical_fields = defaultdict(dict)\n        \n        for model_name, output in model_outputs.items():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                for field, value in output.result.items():\n                    if isinstance(value, str) and not self._is_numerical_string(value):\n                        categorical_fields[field][model_name] = value\n        \n        # Check for conflicts\n        for field, model_values in categorical_fields.items():\n            if len(model_values) > 1:\n                unique_values = set(model_values.values())\n                \n                if len(unique_values) > 1:\n                    # Calculate conflict severity based on disagreement level\n                    value_counts = Counter(model_values.values())\n                    most_common_count = value_counts.most_common(1)[0][1]\n                    total_models = len(model_values)\n                    agreement_ratio = most_common_count / total_models\n                    severity = 1.0 - agreement_ratio\n                    \n                    conflict = ConflictDetection(\n                        conflict_id=f\"categorical_{field}_{hash(str(model_values)) % 10000}\",\n                        conflict_type=ConflictType.CATEGORICAL_CONFLICT,\n                        conflicting_models=list(model_values.keys()),\n                        conflict_description=f\"Categorical conflict in field '{field}': {model_values}\",\n                        severity=severity,\n                        affected_fields=[field],\n                        resolution_strategy=\"majority_voting\",\n                        confidence_impact=0.2 * severity,\n                        metadata={\n                            \"values\": model_values,\n                            \"unique_values\": list(unique_values),\n                            \"value_counts\": dict(value_counts),\n                            \"agreement_ratio\": agreement_ratio\n                        }\n                    )\n                    conflicts.append(conflict)\n        \n        return conflicts\n    \n    def _detect_confidence_conflicts(self, model_outputs: Dict[str, ModelOutput]) -> List[ConflictDetection]:\n        \"\"\"Detect confidence mismatches between models\"\"\"\n        \n        conflicts = []\n        \n        # Extract confidence scores\n        confidences = {}\n        for model_name, output in model_outputs.items():\n            if hasattr(output, 'confidence'):\n                confidences[model_name] = output.confidence\n        \n        if len(confidences) > 1:\n            confidence_values = list(confidences.values())\n            mean_confidence = np.mean(confidence_values)\n            std_confidence = np.std(confidence_values)\n            \n            # Consider it a conflict if confidence standard deviation is high\n            if std_confidence > 0.2:\n                conflict = ConflictDetection(\n                    conflict_id=f\"confidence_mismatch_{hash(str(confidences)) % 10000}\",\n                    conflict_type=ConflictType.CONFIDENCE_MISMATCH,\n                    conflicting_models=list(confidences.keys()),\n                    conflict_description=f\"Confidence mismatch: {confidences}\",\n                    severity=min(1.0, std_confidence / 0.5),  # Normalize by max expected std\n                    affected_fields=[\"confidence\"],\n                    resolution_strategy=\"confidence_weighted\",\n                    confidence_impact=std_confidence,\n                    metadata={\n                        \"confidences\": confidences,\n                        \"mean_confidence\": mean_confidence,\n                        \"std_confidence\": std_confidence\n                    }\n                )\n                conflicts.append(conflict)\n        \n        return conflicts\n    \n    def _detect_semantic_conflicts(self, model_outputs: Dict[str, ModelOutput]) -> List[ConflictDetection]:\n        \"\"\"Detect semantic contradictions between text outputs\"\"\"\n        \n        conflicts = []\n        \n        # Extract text fields\n        text_fields = defaultdict(dict)\n        \n        for model_name, output in model_outputs.items():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                for field, value in output.result.items():\n                    if isinstance(value, str) and len(value) > 50:  # Substantial text\n                        text_fields[field][model_name] = value\n        \n        # Check for semantic conflicts\n        for field, model_texts in text_fields.items():\n            if len(model_texts) > 1:\n                texts = list(model_texts.values())\n                \n                try:\n                    # Calculate semantic similarity using TF-IDF\n                    tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)\n                    similarity_matrix = cosine_similarity(tfidf_matrix)\n                    \n                    # Calculate average pairwise similarity\n                    n = len(texts)\n                    total_similarity = 0\n                    pairs = 0\n                    \n                    for i in range(n):\n                        for j in range(i + 1, n):\n                            total_similarity += similarity_matrix[i][j]\n                            pairs += 1\n                    \n                    avg_similarity = total_similarity / pairs if pairs > 0 else 1.0\n                    \n                    # Consider it a conflict if average similarity is low\n                    if avg_similarity < 0.5:\n                        severity = 1.0 - avg_similarity\n                        \n                        conflict = ConflictDetection(\n                            conflict_id=f\"semantic_{field}_{hash(str(model_texts)) % 10000}\",\n                            conflict_type=ConflictType.SEMANTIC_CONTRADICTION,\n                            conflicting_models=list(model_texts.keys()),\n                            conflict_description=f\"Semantic contradiction in field '{field}' (similarity: {avg_similarity:.3f})\",\n                            severity=severity,\n                            affected_fields=[field],\n                            resolution_strategy=\"semantic_merging\",\n                            confidence_impact=0.3 * severity,\n                            metadata={\n                                \"texts\": model_texts,\n                                \"avg_similarity\": avg_similarity,\n                                \"similarity_matrix\": similarity_matrix.tolist()\n                            }\n                        )\n                        conflicts.append(conflict)\n                        \n                except Exception as e:\n                    logger.warning(f\"Error in semantic conflict detection: {e}\")\n        \n        return conflicts\n    \n    def _detect_temporal_conflicts(self, model_outputs: Dict[str, ModelOutput]) -> List[ConflictDetection]:\n        \"\"\"Detect temporal inconsistencies between outputs\"\"\"\n        \n        conflicts = []\n        \n        # Extract temporal information\n        temporal_fields = defaultdict(dict)\n        \n        for model_name, output in model_outputs.items():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                for field, value in output.result.items():\n                    if self._is_temporal_field(field, value):\n                        temporal_fields[field][model_name] = value\n        \n        # Check for temporal conflicts\n        for field, model_values in temporal_fields.items():\n            if len(model_values) > 1:\n                # Convert to timestamps for comparison\n                timestamps = {}\n                for model, value in model_values.items():\n                    try:\n                        if isinstance(value, str):\n                            timestamp = pd.to_datetime(value).timestamp()\n                        elif isinstance(value, datetime):\n                            timestamp = value.timestamp()\n                        else:\n                            timestamp = float(value)\n                        timestamps[model] = timestamp\n                    except:\n                        continue\n                \n                if len(timestamps) > 1:\n                    timestamp_values = list(timestamps.values())\n                    time_range = max(timestamp_values) - min(timestamp_values)\n                    \n                    # Consider it a conflict if time range is significant (> 1 day)\n                    if time_range > 86400:  # 1 day in seconds\n                        severity = min(1.0, time_range / (30 * 86400))  # Normalize by 30 days\n                        \n                        conflict = ConflictDetection(\n                            conflict_id=f\"temporal_{field}_{hash(str(model_values)) % 10000}\",\n                            conflict_type=ConflictType.TEMPORAL_INCONSISTENCY,\n                            conflicting_models=list(timestamps.keys()),\n                            conflict_description=f\"Temporal inconsistency in field '{field}': {model_values}\",\n                            severity=severity,\n                            affected_fields=[field],\n                            resolution_strategy=\"latest_timestamp\",\n                            confidence_impact=0.15 * severity,\n                            metadata={\n                                \"values\": model_values,\n                                \"timestamps\": timestamps,\n                                \"time_range_seconds\": time_range\n                            }\n                        )\n                        conflicts.append(conflict)\n        \n        return conflicts\n    \n    def _detect_logical_conflicts(self, model_outputs: Dict[str, ModelOutput]) -> List[ConflictDetection]:\n        \"\"\"Detect logical inconsistencies between outputs\"\"\"\n        \n        conflicts = []\n        \n        # This is a simplified logical conflict detection\n        # In practice, this would involve more sophisticated logical reasoning\n        \n        # Check for contradictory boolean values\n        boolean_fields = defaultdict(dict)\n        \n        for model_name, output in model_outputs.items():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                for field, value in output.result.items():\n                    if isinstance(value, bool) or (isinstance(value, str) and value.lower() in ['true', 'false', 'yes', 'no']):\n                        boolean_fields[field][model_name] = value\n        \n        # Check for boolean conflicts\n        for field, model_values in boolean_fields.items():\n            if len(model_values) > 1:\n                # Normalize boolean values\n                normalized_values = {}\n                for model, value in model_values.items():\n                    if isinstance(value, bool):\n                        normalized_values[model] = value\n                    elif isinstance(value, str):\n                        normalized_values[model] = value.lower() in ['true', 'yes']\n                \n                unique_values = set(normalized_values.values())\n                \n                if len(unique_values) > 1:\n                    # Boolean conflict detected\n                    true_count = sum(1 for v in normalized_values.values() if v)\n                    false_count = len(normalized_values) - true_count\n                    \n                    # Severity based on how split the decision is\n                    severity = 1.0 - abs(true_count - false_count) / len(normalized_values)\n                    \n                    conflict = ConflictDetection(\n                        conflict_id=f\"logical_{field}_{hash(str(model_values)) % 10000}\",\n                        conflict_type=ConflictType.LOGICAL_INCONSISTENCY,\n                        conflicting_models=list(normalized_values.keys()),\n                        conflict_description=f\"Logical inconsistency in field '{field}': {model_values}\",\n                        severity=severity,\n                        affected_fields=[field],\n                        resolution_strategy=\"majority_voting\",\n                        confidence_impact=0.25 * severity,\n                        metadata={\n                            \"original_values\": model_values,\n                            \"normalized_values\": normalized_values,\n                            \"true_count\": true_count,\n                            \"false_count\": false_count\n                        }\n                    )\n                    conflicts.append(conflict)\n        \n        return conflicts\n    \n    def _weighted_average_synthesis(self, model_outputs: Dict[str, ModelOutput],\n                                  conflicts: List[ConflictDetection],\n                                  params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, float]]:\n        \"\"\"Synthesize responses using weighted averaging\"\"\"\n        \n        weights = params.get('weights', {})\n        default_weight = 1.0 / len(model_outputs)\n        \n        synthesized = {}\n        individual_contributions = {model: 0.0 for model in model_outputs.keys()}\n        \n        # Collect all fields\n        all_fields = set()\n        for output in model_outputs.values():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                all_fields.update(output.result.keys())\n        \n        # Synthesize each field\n        for field in all_fields:\n            field_values = {}\n            field_weights = {}\n            \n            for model_name, output in model_outputs.items():\n                if (hasattr(output, 'result') and \n                    isinstance(output.result, dict) and \n                    field in output.result):\n                    \n                    value = output.result[field]\n                    weight = weights.get(model_name, default_weight)\n                    \n                    field_values[model_name] = value\n                    field_weights[model_name] = weight\n            \n            if field_values:\n                synthesized_value = self._synthesize_field_weighted(\n                    field_values, field_weights\n                )\n                synthesized[field] = synthesized_value\n                \n                # Update contributions\n                total_weight = sum(field_weights.values())\n                for model_name, weight in field_weights.items():\n                    individual_contributions[model_name] += weight / total_weight / len(all_fields)\n        \n        return synthesized, individual_contributions\n    \n    def _confidence_weighted_synthesis(self, model_outputs: Dict[str, ModelOutput],\n                                     conflicts: List[ConflictDetection],\n                                     params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, float]]:\n        \"\"\"Synthesize responses using confidence-based weighting\"\"\"\n        \n        synthesized = {}\n        individual_contributions = {model: 0.0 for model in model_outputs.keys()}\n        \n        # Calculate confidence weights\n        confidence_weights = {}\n        total_confidence = 0.0\n        \n        for model_name, output in model_outputs.items():\n            confidence = getattr(output, 'confidence', 0.5)\n            confidence_weights[model_name] = confidence\n            total_confidence += confidence\n        \n        # Normalize weights\n        if total_confidence > 0:\n            for model_name in confidence_weights:\n                confidence_weights[model_name] /= total_confidence\n        else:\n            # Fallback to equal weights\n            equal_weight = 1.0 / len(model_outputs)\n            confidence_weights = {model: equal_weight for model in model_outputs.keys()}\n        \n        # Collect all fields\n        all_fields = set()\n        for output in model_outputs.values():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                all_fields.update(output.result.keys())\n        \n        # Synthesize each field\n        for field in all_fields:\n            field_values = {}\n            field_weights = {}\n            \n            for model_name, output in model_outputs.items():\n                if (hasattr(output, 'result') and \n                    isinstance(output.result, dict) and \n                    field in output.result):\n                    \n                    value = output.result[field]\n                    weight = confidence_weights[model_name]\n                    \n                    field_values[model_name] = value\n                    field_weights[model_name] = weight\n            \n            if field_values:\n                synthesized_value = self._synthesize_field_weighted(\n                    field_values, field_weights\n                )\n                synthesized[field] = synthesized_value\n                \n                # Update contributions\n                total_weight = sum(field_weights.values())\n                for model_name, weight in field_weights.items():\n                    individual_contributions[model_name] += weight / total_weight / len(all_fields)\n        \n        return synthesized, individual_contributions\n    \n    def _majority_voting_synthesis(self, model_outputs: Dict[str, ModelOutput],\n                                 conflicts: List[ConflictDetection],\n                                 params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, float]]:\n        \"\"\"Synthesize responses using majority voting\"\"\"\n        \n        synthesized = {}\n        individual_contributions = {model: 0.0 for model in model_outputs.keys()}\n        \n        # Collect all fields\n        all_fields = set()\n        for output in model_outputs.values():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                all_fields.update(output.result.keys())\n        \n        # Vote on each field\n        for field in all_fields:\n            field_values = []\n            contributing_models = []\n            \n            for model_name, output in model_outputs.items():\n                if (hasattr(output, 'result') and \n                    isinstance(output.result, dict) and \n                    field in output.result):\n                    \n                    field_values.append(output.result[field])\n                    contributing_models.append(model_name)\n            \n            if field_values:\n                # For numerical values, use median\n                if all(isinstance(v, (int, float)) for v in field_values):\n                    synthesized_value = np.median(field_values)\n                else:\n                    # For categorical values, use most common\n                    value_counts = Counter(field_values)\n                    synthesized_value = value_counts.most_common(1)[0][0]\n                \n                synthesized[field] = synthesized_value\n                \n                # Equal contribution for all participating models\n                contribution = 1.0 / len(contributing_models) / len(all_fields)\n                for model_name in contributing_models:\n                    individual_contributions[model_name] += contribution\n        \n        return synthesized, individual_contributions\n"    \n  
  def _consensus_building_synthesis(self, model_outputs: Dict[str, ModelOutput],\n                                    conflicts: List[ConflictDetection],\n                                    params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, float]]:\n        \"\"\"Synthesize responses by building consensus\"\"\"\n        \n        consensus_threshold = params.get('consensus_threshold', 0.6)\n        \n        synthesized = {}\n        individual_contributions = {model: 0.0 for model in model_outputs.keys()}\n        \n        # Collect all fields\n        all_fields = set()\n        for output in model_outputs.values():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                all_fields.update(output.result.keys())\n        \n        # Build consensus for each field\n        for field in all_fields:\n            field_values = []\n            contributing_models = []\n            \n            for model_name, output in model_outputs.items():\n                if (hasattr(output, 'result') and \n                    isinstance(output.result, dict) and \n                    field in output.result):\n                    \n                    field_values.append(output.result[field])\n                    contributing_models.append(model_name)\n            \n            if field_values:\n                consensus_value, consensus_models = self._find_consensus(\n                    field_values, contributing_models, consensus_threshold\n                )\n                \n                if consensus_value is not None:\n                    synthesized[field] = consensus_value\n                    \n                    # Higher contribution for consensus models\n                    consensus_contribution = 0.8 / len(consensus_models) / len(all_fields)\n                    minority_contribution = 0.2 / (len(contributing_models) - len(consensus_models)) / len(all_fields) if len(contributing_models) > len(consensus_models) else 0\n                    \n                    for model_name in contributing_models:\n                        if model_name in consensus_models:\n                            individual_contributions[model_name] += consensus_contribution\n                        else:\n                            individual_contributions[model_name] += minority_contribution\n                else:\n                    # No consensus reached, fall back to weighted average\n                    if all(isinstance(v, (int, float)) for v in field_values):\n                        synthesized[field] = np.mean(field_values)\n                    else:\n                        # Use most common value\n                        value_counts = Counter(field_values)\n                        synthesized[field] = value_counts.most_common(1)[0][0]\n                    \n                    # Equal contribution when no consensus\n                    contribution = 1.0 / len(contributing_models) / len(all_fields)\n                    for model_name in contributing_models:\n                        individual_contributions[model_name] += contribution\n        \n        return synthesized, individual_contributions\n    \n    def _hierarchical_fusion_synthesis(self, model_outputs: Dict[str, ModelOutput],\n                                     conflicts: List[ConflictDetection],\n                                     params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, float]]:\n        \"\"\"Synthesize responses using hierarchical fusion\"\"\"\n        \n        model_hierarchy = params.get('model_hierarchy', {})\n        \n        synthesized = {}\n        individual_contributions = {model: 0.0 for model in model_outputs.keys()}\n        \n        # Create hierarchy levels\n        hierarchy_levels = defaultdict(list)\n        for model_name in model_outputs.keys():\n            level = model_hierarchy.get(model_name, 1)  # Default to level 1\n            hierarchy_levels[level].append(model_name)\n        \n        # Sort levels (higher level = higher priority)\n        sorted_levels = sorted(hierarchy_levels.keys(), reverse=True)\n        \n        # Collect all fields\n        all_fields = set()\n        for output in model_outputs.values():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                all_fields.update(output.result.keys())\n        \n        # Synthesize each field hierarchically\n        for field in all_fields:\n            field_synthesized = False\n            \n            for level in sorted_levels:\n                level_models = hierarchy_levels[level]\n                level_values = {}\n                \n                for model_name in level_models:\n                    output = model_outputs[model_name]\n                    if (hasattr(output, 'result') and \n                        isinstance(output.result, dict) and \n                        field in output.result):\n                        level_values[model_name] = output.result[field]\n                \n                if level_values:\n                    # Synthesize within this level\n                    if len(level_values) == 1:\n                        # Single model at this level\n                        model_name = list(level_values.keys())[0]\n                        synthesized[field] = level_values[model_name]\n                        individual_contributions[model_name] += 1.0 / len(all_fields)\n                    else:\n                        # Multiple models at this level - use weighted average\n                        level_weights = {model: 1.0 / len(level_values) for model in level_values.keys()}\n                        synthesized_value = self._synthesize_field_weighted(level_values, level_weights)\n                        synthesized[field] = synthesized_value\n                        \n                        # Distribute contributions\n                        for model_name in level_values.keys():\n                            individual_contributions[model_name] += 1.0 / len(level_values) / len(all_fields)\n                    \n                    field_synthesized = True\n                    break\n            \n            # If no model had this field, skip it\n            if not field_synthesized:\n                logger.warning(f\"No model provided value for field: {field}\")\n        \n        return synthesized, individual_contributions\n    \n    def _semantic_merging_synthesis(self, model_outputs: Dict[str, ModelOutput],\n                                  conflicts: List[ConflictDetection],\n                                  params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, float]]:\n        \"\"\"Synthesize responses using semantic merging\"\"\"\n        \n        synthesized = {}\n        individual_contributions = {model: 0.0 for model in model_outputs.keys()}\n        \n        # Collect all fields\n        all_fields = set()\n        for output in model_outputs.values():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                all_fields.update(output.result.keys())\n        \n        # Semantically merge each field\n        for field in all_fields:\n            field_values = {}\n            \n            for model_name, output in model_outputs.items():\n                if (hasattr(output, 'result') and \n                    isinstance(output.result, dict) and \n                    field in output.result):\n                    field_values[model_name] = output.result[field]\n            \n            if field_values:\n                if len(field_values) == 1:\n                    # Single value, use as-is\n                    model_name = list(field_values.keys())[0]\n                    synthesized[field] = field_values[model_name]\n                    individual_contributions[model_name] += 1.0 / len(all_fields)\n                else:\n                    # Multiple values, merge semantically\n                    merged_value = self._semantic_merge_values(field_values)\n                    synthesized[field] = merged_value\n                    \n                    # Equal contribution for semantic merging\n                    contribution = 1.0 / len(field_values) / len(all_fields)\n                    for model_name in field_values.keys():\n                        individual_contributions[model_name] += contribution\n        \n        return synthesized, individual_contributions\n    \n    def _contradiction_resolution_synthesis(self, model_outputs: Dict[str, ModelOutput],\n                                          conflicts: List[ConflictDetection],\n                                          params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, float]]:\n        \"\"\"Synthesize responses by explicitly resolving contradictions\"\"\"\n        \n        synthesized = {}\n        individual_contributions = {model: 0.0 for model in model_outputs.keys()}\n        \n        # Start with confidence-weighted synthesis as base\n        base_synthesized, base_contributions = self._confidence_weighted_synthesis(\n            model_outputs, conflicts, params\n        )\n        \n        synthesized.update(base_synthesized)\n        individual_contributions.update(base_contributions)\n        \n        # Resolve each conflict\n        for conflict in conflicts:\n            if conflict.severity > 0.5:  # Only resolve high-severity conflicts\n                resolved_value = self._resolve_specific_conflict(\n                    conflict, model_outputs, synthesized\n                )\n                \n                if resolved_value is not None:\n                    for field in conflict.affected_fields:\n                        synthesized[field] = resolved_value\n                        \n                        # Adjust contributions based on conflict resolution\n                        self._adjust_contributions_for_conflict(\n                            individual_contributions, conflict, len(synthesized)\n                        )\n        \n        return synthesized, individual_contributions\n    \n    def _narrative_synthesis(self, model_outputs: Dict[str, ModelOutput],\n                           conflicts: List[ConflictDetection],\n                           params: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, float]]:\n        \"\"\"Synthesize responses into a coherent narrative\"\"\"\n        \n        synthesized = {}\n        individual_contributions = {model: 0.0 for model in model_outputs.keys()}\n        \n        # Extract text content from all models\n        text_content = {}\n        numerical_content = {}\n        \n        for model_name, output in model_outputs.items():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                for field, value in output.result.items():\n                    if isinstance(value, str) and len(value) > 20:\n                        if field not in text_content:\n                            text_content[field] = {}\n                        text_content[field][model_name] = value\n                    elif isinstance(value, (int, float)):\n                        if field not in numerical_content:\n                            numerical_content[field] = {}\n                        numerical_content[field][model_name] = value\n        \n        # Synthesize numerical content using confidence weighting\n        for field, model_values in numerical_content.items():\n            weights = {}\n            for model_name in model_values.keys():\n                output = model_outputs[model_name]\n                weights[model_name] = getattr(output, 'confidence', 0.5)\n            \n            synthesized_value = self._synthesize_field_weighted(model_values, weights)\n            synthesized[field] = synthesized_value\n        \n        # Synthesize text content into narrative\n        for field, model_texts in text_content.items():\n            narrative = self._create_narrative_from_texts(model_texts, model_outputs)\n            synthesized[field] = narrative\n        \n        # Calculate contributions based on text length and confidence\n        total_fields = len(text_content) + len(numerical_content)\n        if total_fields > 0:\n            for model_name in model_outputs.keys():\n                contribution = 0.0\n                \n                # Text contribution\n                for field, model_texts in text_content.items():\n                    if model_name in model_texts:\n                        text_length = len(model_texts[model_name])\n                        total_text_length = sum(len(text) for text in model_texts.values())\n                        if total_text_length > 0:\n                            contribution += (text_length / total_text_length) / total_fields\n                \n                # Numerical contribution\n                for field, model_values in numerical_content.items():\n                    if model_name in model_values:\n                        contribution += 1.0 / len(model_values) / total_fields\n                \n                individual_contributions[model_name] = contribution\n        \n        return synthesized, individual_contributions\n    \n    def _synthesize_field_weighted(self, field_values: Dict[str, Any], \n                                 weights: Dict[str, float]) -> Any:\n        \"\"\"Synthesize a single field using weighted combination\"\"\"\n        \n        if not field_values:\n            return None\n        \n        if len(field_values) == 1:\n            return list(field_values.values())[0]\n        \n        # Check if all values are numerical\n        numerical_values = []\n        numerical_weights = []\n        \n        for model, value in field_values.items():\n            if isinstance(value, (int, float)):\n                numerical_values.append(value)\n                numerical_weights.append(weights.get(model, 1.0))\n        \n        if len(numerical_values) == len(field_values):\n            # All numerical - compute weighted average\n            total_weight = sum(numerical_weights)\n            if total_weight > 0:\n                weighted_sum = sum(v * w for v, w in zip(numerical_values, numerical_weights))\n                return weighted_sum / total_weight\n            else:\n                return np.mean(numerical_values)\n        \n        # For non-numerical values, use highest weighted value\n        max_weight = -1\n        best_value = None\n        \n        for model, value in field_values.items():\n            weight = weights.get(model, 1.0)\n            if weight > max_weight:\n                max_weight = weight\n                best_value = value\n        \n        return best_value\n    \n    def _find_consensus(self, values: List[Any], models: List[str], \n                       threshold: float) -> Tuple[Any, List[str]]:\n        \"\"\"Find consensus value among models\"\"\"\n        \n        if not values:\n            return None, []\n        \n        # Count occurrences of each value\n        value_counts = Counter(values)\n        total_count = len(values)\n        \n        # Find values that meet consensus threshold\n        for value, count in value_counts.most_common():\n            if count / total_count >= threshold:\n                # Find models that contributed this value\n                consensus_models = []\n                for i, v in enumerate(values):\n                    if v == value:\n                        consensus_models.append(models[i])\n                \n                return value, consensus_models\n        \n        return None, []\n    \n    def _semantic_merge_values(self, field_values: Dict[str, Any]) -> Any:\n        \"\"\"Merge values semantically\"\"\"\n        \n        if not field_values:\n            return None\n        \n        if len(field_values) == 1:\n            return list(field_values.values())[0]\n        \n        # For text values, create a merged narrative\n        text_values = []\n        for value in field_values.values():\n            if isinstance(value, str):\n                text_values.append(value)\n        \n        if text_values:\n            # Simple semantic merging - combine unique sentences\n            all_sentences = []\n            for text in text_values:\n                sentences = re.split(r'[.!?]+', text)\n                for sentence in sentences:\n                    sentence = sentence.strip()\n                    if sentence and sentence not in all_sentences:\n                        all_sentences.append(sentence)\n            \n            return '. '.join(all_sentences) + '.'\n        \n        # For non-text values, return the first one\n        return list(field_values.values())[0]\n    \n    def _create_narrative_from_texts(self, model_texts: Dict[str, str], \n                                   model_outputs: Dict[str, ModelOutput]) -> str:\n        \"\"\"Create coherent narrative from multiple text sources\"\"\"\n        \n        if not model_texts:\n            return \"\"\n        \n        if len(model_texts) == 1:\n            return list(model_texts.values())[0]\n        \n        # Sort texts by model confidence\n        sorted_texts = []\n        for model_name, text in model_texts.items():\n            confidence = getattr(model_outputs[model_name], 'confidence', 0.5)\n            sorted_texts.append((confidence, text, model_name))\n        \n        sorted_texts.sort(key=lambda x: x[0], reverse=True)\n        \n        # Create narrative by combining high-confidence content\n        narrative_parts = []\n        used_content = set()\n        \n        for confidence, text, model_name in sorted_texts:\n            # Extract unique sentences\n            sentences = re.split(r'[.!?]+', text)\n            for sentence in sentences:\n                sentence = sentence.strip()\n                if sentence and sentence not in used_content:\n                    narrative_parts.append(sentence)\n                    used_content.add(sentence)\n        \n        return '. '.join(narrative_parts) + '.' if narrative_parts else \"\"\n    \n    def _resolve_specific_conflict(self, conflict: ConflictDetection,\n                                 model_outputs: Dict[str, ModelOutput],\n                                 current_synthesis: Dict[str, Any]) -> Any:\n        \"\"\"Resolve a specific conflict\"\"\"\n        \n        if conflict.conflict_type == ConflictType.NUMERICAL_DISAGREEMENT:\n            # Use confidence-weighted average\n            values = conflict.metadata['values']\n            weights = {}\n            for model_name in values.keys():\n                output = model_outputs[model_name]\n                weights[model_name] = getattr(output, 'confidence', 0.5)\n            \n            return self._synthesize_field_weighted(values, weights)\n        \n        elif conflict.conflict_type == ConflictType.CATEGORICAL_CONFLICT:\n            # Use majority voting with confidence tie-breaking\n            values = conflict.metadata['values']\n            value_counts = Counter(values.values())\n            \n            # If there's a clear majority, use it\n            most_common = value_counts.most_common(2)\n            if len(most_common) == 1 or most_common[0][1] > most_common[1][1]:\n                return most_common[0][0]\n            \n            # Tie-breaking using confidence\n            tied_values = [item[0] for item in most_common if item[1] == most_common[0][1]]\n            best_confidence = -1\n            best_value = tied_values[0]\n            \n            for model_name, value in values.items():\n                if value in tied_values:\n                    confidence = getattr(model_outputs[model_name], 'confidence', 0.5)\n                    if confidence > best_confidence:\n                        best_confidence = confidence\n                        best_value = value\n            \n            return best_value\n        \n        elif conflict.conflict_type == ConflictType.CONFIDENCE_MISMATCH:\n            # Use highest confidence model's value\n            confidences = conflict.metadata['confidences']\n            highest_confidence_model = max(confidences.items(), key=lambda x: x[1])[0]\n            \n            output = model_outputs[highest_confidence_model]\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                field = conflict.affected_fields[0]\n                if field in output.result:\n                    return output.result[field]\n        \n        return None\n    \n    def _adjust_contributions_for_conflict(self, contributions: Dict[str, float],\n                                         conflict: ConflictDetection, total_fields: int):\n        \"\"\"Adjust model contributions based on conflict resolution\"\"\"\n        \n        # Reduce contributions for models involved in high-severity conflicts\n        penalty = conflict.severity * 0.1 / total_fields\n        \n        for model_name in conflict.conflicting_models:\n            contributions[model_name] = max(0.0, contributions[model_name] - penalty)\n    \n    def _quantify_uncertainty(self, model_outputs: Dict[str, ModelOutput],\n                            conflicts: List[ConflictDetection],\n                            synthesized_response: Dict[str, Any]) -> Dict[str, float]:\n        \"\"\"Quantify different types of uncertainty in the synthesis\"\"\"\n        \n        uncertainty = {\n            UncertaintyType.ALEATORY.value: 0.0,\n            UncertaintyType.EPISTEMIC.value: 0.0,\n            UncertaintyType.MODEL_UNCERTAINTY.value: 0.0,\n            UncertaintyType.DATA_UNCERTAINTY.value: 0.0\n        }\n        \n        # Model uncertainty based on confidence variance\n        confidences = []\n        for output in model_outputs.values():\n            if hasattr(output, 'confidence'):\n                confidences.append(output.confidence)\n        \n        if confidences:\n            confidence_std = np.std(confidences)\n            uncertainty[UncertaintyType.MODEL_UNCERTAINTY.value] = min(1.0, confidence_std * 2)\n        \n        # Epistemic uncertainty based on conflicts\n        if conflicts:\n            avg_conflict_severity = np.mean([c.severity for c in conflicts])\n            uncertainty[UncertaintyType.EPISTEMIC.value] = avg_conflict_severity\n        \n        # Data uncertainty based on input consistency (simplified)\n        uncertainty[UncertaintyType.DATA_UNCERTAINTY.value] = 0.1  # Placeholder\n        \n        # Aleatory uncertainty (inherent randomness) - simplified\n        uncertainty[UncertaintyType.ALEATORY.value] = 0.05  # Placeholder\n        \n        return uncertainty\n    \n    def _calculate_synthesis_confidence(self, model_outputs: Dict[str, ModelOutput],\n                                      conflicts: List[ConflictDetection],\n                                      individual_contributions: Dict[str, float]) -> float:\n        \"\"\"Calculate overall confidence in the synthesis\"\"\"\n        \n        # Base confidence from individual model confidences\n        base_confidence = 0.0\n        total_weight = 0.0\n        \n        for model_name, output in model_outputs.items():\n            if hasattr(output, 'confidence'):\n                contribution = individual_contributions.get(model_name, 0.0)\n                base_confidence += output.confidence * contribution\n                total_weight += contribution\n        \n        if total_weight > 0:\n            base_confidence /= total_weight\n        else:\n            base_confidence = 0.5  # Default\n        \n        # Reduce confidence based on conflicts\n        conflict_penalty = 0.0\n        if conflicts:\n            total_conflict_impact = sum(c.confidence_impact for c in conflicts)\n            conflict_penalty = min(0.5, total_conflict_impact)  # Cap at 50% reduction\n        \n        # Boost confidence if models agree\n        agreement_bonus = 0.0\n        if len(model_outputs) > 1:\n            # Calculate agreement based on lack of high-severity conflicts\n            high_severity_conflicts = [c for c in conflicts if c.severity > 0.7]\n            if not high_severity_conflicts:\n                agreement_bonus = 0.1  # 10% bonus for agreement\n        \n        final_confidence = base_confidence - conflict_penalty + agreement_bonus\n        return max(0.0, min(1.0, final_confidence))\n    \n    def _resolve_conflicts(self, conflicts: List[ConflictDetection],\n                         synthesized_response: Dict[str, Any],\n                         strategy: SynthesisStrategy) -> List[Dict[str, Any]]:\n        \"\"\"Resolve detected conflicts\"\"\"\n        \n        resolved_conflicts = []\n        \n        for conflict in conflicts:\n            resolution = {\n                \"conflict_id\": conflict.conflict_id,\n                \"conflict_type\": conflict.conflict_type.value,\n                \"resolution_strategy\": conflict.resolution_strategy,\n                \"resolution_applied\": True,\n                \"resolution_confidence\": 1.0 - conflict.severity,\n                \"affected_fields\": conflict.affected_fields,\n                \"resolution_method\": strategy.value\n            }\n            \n            resolved_conflicts.append(resolution)\n        \n        return resolved_conflicts\n    \n    def _calculate_quality_metrics(self, model_outputs: Dict[str, ModelOutput],\n                                 synthesized_response: Dict[str, Any],\n                                 conflicts: List[ConflictDetection]) -> Dict[str, float]:\n        \"\"\"Calculate quality metrics for the synthesis\"\"\"\n        \n        metrics = {}\n        \n        # Completeness: How many fields were successfully synthesized\n        total_possible_fields = set()\n        for output in model_outputs.values():\n            if hasattr(output, 'result') and isinstance(output.result, dict):\n                total_possible_fields.update(output.result.keys())\n        \n        if total_possible_fields:\n            completeness = len(synthesized_response) / len(total_possible_fields)\n            metrics['completeness'] = completeness\n        else:\n            metrics['completeness'] = 0.0\n        \n        # Consistency: Inverse of conflict severity\n        if conflicts:\n            avg_conflict_severity = np.mean([c.severity for c in conflicts])\n            consistency = 1.0 - avg_conflict_severity\n        else:\n            consistency = 1.0\n        metrics['consistency'] = consistency\n        \n        # Confidence: Average model confidence weighted by contributions\n        # (This would be calculated in the main synthesis function)\n        metrics['confidence'] = 0.8  # Placeholder\n        \n        # Robustness: Based on number of contributing models\n        num_models = len(model_outputs)\n        robustness = min(1.0, num_models / 3.0)  # Normalize by expected 3 models\n        metrics['robustness'] = robustness\n        \n        return metrics\n    \n    def _update_performance_metrics(self, result: SynthesisResult):\n        \"\"\"Update performance tracking metrics\"\"\"\n        \n        strategy = result.synthesis_strategy.value\n        \n        self.performance_metrics[strategy].append({\n            'timestamp': result.timestamp,\n            'confidence': result.confidence_score,\n            'num_conflicts': len(result.conflicts_detected),\n            'synthesis_time': result.synthesis_metadata['synthesis_time'],\n            'quality_metrics': result.quality_metrics\n        })\n        \n        # Keep only last 1000 entries per strategy\n        if len(self.performance_metrics[strategy]) > 1000:\n            self.performance_metrics[strategy] = self.performance_metrics[strategy][-1000:]\n    \n    def _is_numerical_string(self, value: str) -> bool:\n        \"\"\"Check if a string represents a numerical value\"\"\"\n        try:\n            float(value)\n            return True\n        except ValueError:\n            return False\n    \n    def _is_temporal_field(self, field_name: str, value: Any) -> bool:\n        \"\"\"Check if a field contains temporal information\"\"\"\n        \n        temporal_keywords = ['time', 'date', 'timestamp', 'created', 'updated', 'when']\n        \n        # Check field name\n        field_lower = field_name.lower()\n        if any(keyword in field_lower for keyword in temporal_keywords):\n            return True\n        \n        # Check value format\n        if isinstance(value, str):\n            # Simple date/time pattern matching\n            date_patterns = [\n                r'\\d{4}-\\d{2}-\\d{2}',  # YYYY-MM-DD\n                r'\\d{2}/\\d{2}/\\d{4}',  # MM/DD/YYYY\n                r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}',  # ISO format\n            ]\n            \n            for pattern in date_patterns:\n                if re.search(pattern, value):\n                    return True\n        \n        return False\n    \n    def get_synthesis_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get synthesis performance statistics\"\"\"\n        \n        stats = {\n            'total_syntheses': len(self.synthesis_history),\n            'strategy_usage': {},\n            'average_confidence': 0.0,\n            'average_conflicts': 0.0,\n            'performance_by_strategy': {}\n        }\n        \n        if not self.synthesis_history:\n            return stats\n        \n        # Strategy usage\n        strategy_counts = Counter(result.synthesis_strategy.value for result in self.synthesis_history)\n        stats['strategy_usage'] = dict(strategy_counts)\n        \n        # Average metrics\n        total_confidence = sum(result.confidence_score for result in self.synthesis_history)\n        stats['average_confidence'] = total_confidence / len(self.synthesis_history)\n        \n        total_conflicts = sum(len(result.conflicts_detected) for result in self.synthesis_history)\n        stats['average_conflicts'] = total_conflicts / len(self.synthesis_history)\n        \n        # Performance by strategy\n        for strategy, metrics_list in self.performance_metrics.items():\n            if metrics_list:\n                avg_confidence = np.mean([m['confidence'] for m in metrics_list])\n                avg_conflicts = np.mean([m['num_conflicts'] for m in metrics_list])\n                avg_time = np.mean([m['synthesis_time'] for m in metrics_list])\n                \n                stats['performance_by_strategy'][strategy] = {\n                    'average_confidence': avg_confidence,\n                    'average_conflicts': avg_conflicts,\n                    'average_synthesis_time': avg_time,\n                    'sample_count': len(metrics_list)\n                }\n        \n        return stats\n    \n    def recommend_synthesis_strategy(self, model_outputs: Dict[str, ModelOutput],\n                                   requirements: Dict[str, Any] = None) -> SynthesisStrategy:\n        \"\"\"Recommend optimal synthesis strategy based on inputs and requirements\"\"\"\n        \n        requirements = requirements or {}\n        \n        # Analyze model outputs\n        num_models = len(model_outputs)\n        confidence_variance = 0.0\n        \n        confidences = []\n        for output in model_outputs.values():\n            if hasattr(output, 'confidence'):\n                confidences.append(output.confidence)\n        \n        if confidences:\n            confidence_variance = np.var(confidences)\n        \n        # Decision logic\n        if requirements.get('prioritize_speed', False):\n            return SynthesisStrategy.MAJORITY_VOTING\n        \n        if requirements.get('prioritize_accuracy', False):\n            if confidence_variance > 0.1:\n                return SynthesisStrategy.CONFIDENCE_WEIGHTED\n            else:\n                return SynthesisStrategy.CONSENSUS_BUILDING\n        \n        if requirements.get('handle_conflicts', False):\n            return SynthesisStrategy.CONTRADICTION_RESOLUTION\n        \n        if requirements.get('narrative_output', False):\n            return SynthesisStrategy.NARRATIVE_SYNTHESIS\n        \n        if num_models > 3:\n            return SynthesisStrategy.HIERARCHICAL_FUSION\n        \n        # Default recommendation\n        return SynthesisStrategy.CONFIDENCE_WEIGHTED\n\n\n# Factory functions for easy creation\ndef create_response_synthesizer() -> ResponseSynthesizer:\n    \"\"\"Factory function to create a response synthesizer\"\"\"\n    return ResponseSynthesizer()\n\n\ndef synthesize_model_outputs(model_outputs: Dict[str, ModelOutput],\n                            strategy: SynthesisStrategy = SynthesisStrategy.CONFIDENCE_WEIGHTED,\n                            **kwargs) -> SynthesisResult:\n    \"\"\"Convenience function to synthesize model outputs\"\"\"\n    \n    synthesizer = ResponseSynthesizer()\n    return synthesizer.synthesize_responses(model_outputs, strategy, kwargs)\n